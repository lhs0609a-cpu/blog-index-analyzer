"""
Top Posts Analysis Router - ìƒìœ„ ê¸€ íŒ¨í„´ ë¶„ì„ API

ì´ ëª¨ë“ˆì€ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìƒìœ„ ë…¸ì¶œ ê¸€ë“¤ì˜ íŒ¨í„´ì„ ë¶„ì„í•˜ê³ ,
ì¶•ì ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì‹œê°„ ê¸€ì“°ê¸° ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

í•µì‹¬ ê¸°ëŠ¥:
1. í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œ ìƒìœ„ 1~3ìœ„ ê¸€ ìžë™ ë¶„ì„
2. ë¶„ì„ ê²°ê³¼ DB ì €ìž¥ ë° íŒ¨í„´ ì¶•ì 
3. ì¶•ì ëœ ë°ì´í„° ê¸°ë°˜ ì‹¤ì‹œê°„ ê¸€ì“°ê¸° ê°€ì´ë“œ ìƒì„±
"""
from fastapi import APIRouter, HTTPException, Query, BackgroundTasks
from pydantic import BaseModel
from typing import List, Dict, Optional, Any
import logging
import asyncio
from datetime import datetime

from database.top_posts_db import (
    init_top_posts_tables,
    save_post_analysis,
    get_analysis_count,
    get_category_stats,
    update_aggregated_patterns,
    get_aggregated_patterns,
    get_all_patterns,
    generate_writing_rules,
    get_recent_analyses,
    detect_category
)

# blogs.pyì˜ ë¶„ì„ í•¨ìˆ˜ import
from routers.blogs import analyze_post, fetch_naver_search_results

router = APIRouter()
logger = logging.getLogger(__name__)

# Initialize tables on module load
try:
    init_top_posts_tables()
except Exception as e:
    logger.warning(f"Could not initialize top_posts tables: {e}")


# ===== Request/Response Models =====

class PostAnalysisResult(BaseModel):
    keyword: str
    rank: int
    blog_id: str
    post_url: str
    title_length: int = 0
    title_has_keyword: bool = False
    title_keyword_position: int = -1
    content_length: int = 0
    image_count: int = 0
    video_count: int = 0
    heading_count: int = 0
    keyword_count: int = 0
    keyword_density: float = 0
    has_map: bool = False
    has_link: bool = False
    like_count: int = 0
    comment_count: int = 0
    post_age_days: Optional[int] = None
    category: str = "general"
    data_quality: str = "low"


class AnalyzeTopPostsRequest(BaseModel):
    keyword: str
    top_n: int = 3  # ìƒìœ„ ëª‡ ê°œ ë¶„ì„í• ì§€


class AnalyzeTopPostsResponse(BaseModel):
    keyword: str
    category: str
    analyzed_count: int
    results: List[PostAnalysisResult]
    message: str


class AggregatedPatternResponse(BaseModel):
    category: str
    sample_count: int
    patterns: Dict[str, Any]
    confidence: float
    updated_at: Optional[str] = None


class WritingGuideResponse(BaseModel):
    status: str  # 'data_driven' or 'insufficient_data'
    category: str
    sample_count: int
    confidence: float
    message: Optional[str] = None
    rules: Dict[str, Any]
    updated_at: Optional[str] = None


class AnalysisStatsResponse(BaseModel):
    total_analyses: int
    category_breakdown: Dict[str, int]
    recent_analyses: List[Dict[str, Any]]


# ===== API Endpoints =====

@router.post("/analyze", response_model=AnalyzeTopPostsResponse)
async def analyze_top_posts(request: AnalyzeTopPostsRequest, background_tasks: BackgroundTasks):
    """
    í‚¤ì›Œë“œì˜ ìƒìœ„ ê¸€ë“¤ì„ ë¶„ì„í•˜ê³  íŒ¨í„´ì„ ì €ìž¥í•©ë‹ˆë‹¤.

    - ìƒìœ„ 1~3ìœ„ ê¸€ì˜ êµ¬ì¡°ë¥¼ ë¶„ì„
    - ì œëª© ê¸¸ì´, í‚¤ì›Œë“œ ìœ„ì¹˜, ë³¸ë¬¸ ê¸¸ì´, ì´ë¯¸ì§€ ìˆ˜ ë“± ì¶”ì¶œ
    - ë¶„ì„ ê²°ê³¼ë¥¼ DBì— ì €ìž¥í•˜ì—¬ íŒ¨í„´ ì¶•ì 
    """
    keyword = request.keyword.strip()
    top_n = min(request.top_n, 5)  # ìµœëŒ€ 5ê°œê¹Œì§€

    logger.info(f"Analyzing top {top_n} posts for keyword: {keyword}")

    # ì¹´í…Œê³ ë¦¬ ìžë™ ê°ì§€
    category = detect_category(keyword)

    try:
        # 1. ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        search_results = await fetch_naver_search_results(keyword, limit=top_n)

        if not search_results:
            return AnalyzeTopPostsResponse(
                keyword=keyword,
                category=category,
                analyzed_count=0,
                results=[],
                message="ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )

        # 2. ìƒìœ„ ê¸€ë“¤ ë¶„ì„
        results = []
        for item in search_results[:top_n]:
            try:
                # ê°œë³„ í¬ìŠ¤íŠ¸ ë¶„ì„
                post_analysis = await analyze_post(item['post_url'], keyword)

                # ë¶„ì„ ê²°ê³¼ êµ¬ì„±
                analysis_data = {
                    'keyword': keyword,
                    'rank': item['rank'],
                    'blog_id': item['blog_id'],
                    'post_url': item['post_url'],
                    'title_length': len(item.get('post_title', '')),
                    'title_has_keyword': post_analysis.get('title_has_keyword', False),
                    'title_keyword_position': post_analysis.get('title_keyword_position', -1),
                    'content_length': post_analysis.get('content_length', 0),
                    'image_count': post_analysis.get('image_count', 0),
                    'video_count': post_analysis.get('video_count', 0),
                    'heading_count': post_analysis.get('heading_count', 0),
                    'paragraph_count': post_analysis.get('paragraph_count', 0),
                    'keyword_count': post_analysis.get('keyword_count', 0),
                    'keyword_density': post_analysis.get('keyword_density', 0),
                    'has_map': post_analysis.get('has_map', False),
                    'has_link': post_analysis.get('has_link', False),
                    'like_count': post_analysis.get('like_count', 0),
                    'comment_count': post_analysis.get('comment_count', 0),
                    'post_age_days': post_analysis.get('post_age_days'),
                    'category': category,
                    'data_quality': 'high' if post_analysis.get('data_fetched') else 'low'
                }

                # DBì— ì €ìž¥
                save_post_analysis(analysis_data)

                results.append(PostAnalysisResult(**analysis_data))

                logger.info(f"Analyzed rank {item['rank']}: {item['blog_id']} - {analysis_data['content_length']} chars, {analysis_data['image_count']} images")

            except Exception as e:
                logger.warning(f"Failed to analyze post {item['post_url']}: {e}")

        # 3. ë°±ê·¸ë¼ìš´ë“œì—ì„œ íŒ¨í„´ ì§‘ê³„ ì—…ë°ì´íŠ¸
        background_tasks.add_task(update_aggregated_patterns, category)
        background_tasks.add_task(update_aggregated_patterns, 'general')

        return AnalyzeTopPostsResponse(
            keyword=keyword,
            category=category,
            analyzed_count=len(results),
            results=results,
            message=f"{len(results)}ê°œ ìƒìœ„ ê¸€ ë¶„ì„ ì™„ë£Œ. ì¹´í…Œê³ ë¦¬: {category}"
        )

    except Exception as e:
        logger.error(f"Error analyzing top posts for {keyword}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/patterns/{category}", response_model=AggregatedPatternResponse)
async def get_patterns(category: str = "general"):
    """
    íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ì§‘ê³„ëœ íŒ¨í„´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    ì¹´í…Œê³ ë¦¬:
    - general: ì „ì²´
    - hospital: ë³‘ì›/ì˜ë£Œ
    - restaurant: ë§›ì§‘/ìŒì‹ì 
    - beauty: ë·°í‹°/í™”ìž¥í’ˆ
    - parenting: ìœ¡ì•„/êµìœ¡
    - travel: ì—¬í–‰/ìˆ™ì†Œ
    - tech: IT/ë¦¬ë·°
    """
    patterns = get_aggregated_patterns(category)

    if not patterns:
        # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ general ì‚¬ìš©
        patterns = get_aggregated_patterns('general')

    if not patterns:
        return AggregatedPatternResponse(
            category=category,
            sample_count=0,
            patterns={},
            confidence=0,
            message="ì•„ì§ ë¶„ì„ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        )

    sample_count = patterns.get('sample_count', 0)
    confidence = min(1.0, sample_count / 100)

    return AggregatedPatternResponse(
        category=category,
        sample_count=sample_count,
        patterns={
            "title": {
                "avg_length": patterns.get('avg_title_length', 0),
                "keyword_rate": patterns.get('title_keyword_rate', 0) * 100,
                "keyword_position": {
                    "front": patterns.get('keyword_position_front', 0) * 100,
                    "middle": patterns.get('keyword_position_middle', 0) * 100,
                    "end": patterns.get('keyword_position_end', 0) * 100
                }
            },
            "content": {
                "avg_length": patterns.get('avg_content_length', 0),
                "min_length": patterns.get('min_content_length', 0),
                "max_length": patterns.get('max_content_length', 0),
                "optimal_range": {
                    "min": patterns.get('optimal_content_min', 0),
                    "max": patterns.get('optimal_content_max', 0)
                },
                "avg_heading_count": patterns.get('avg_heading_count', 0),
                "avg_keyword_count": patterns.get('avg_keyword_count', 0),
                "avg_keyword_density": patterns.get('avg_keyword_density', 0)
            },
            "media": {
                "avg_image_count": patterns.get('avg_image_count', 0),
                "optimal_image_range": {
                    "min": patterns.get('optimal_image_min', 0),
                    "max": patterns.get('optimal_image_max', 0)
                },
                "avg_video_count": patterns.get('avg_video_count', 0),
                "video_usage_rate": patterns.get('video_usage_rate', 0) * 100
            },
            "extras": {
                "map_usage_rate": patterns.get('map_usage_rate', 0) * 100,
                "link_usage_rate": patterns.get('link_usage_rate', 0) * 100
            }
        },
        confidence=round(confidence, 2),
        updated_at=patterns.get('updated_at')
    )


@router.get("/writing-guide", response_model=WritingGuideResponse)
async def get_writing_guide(category: str = Query("general", description="ì¹´í…Œê³ ë¦¬")):
    """
    ì‹¤ì‹œê°„ ê¸€ì“°ê¸° ìµœì í™” ê°€ì´ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    ì¶•ì ëœ ìƒìœ„ ê¸€ ë¶„ì„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì í™”ëœ ê¸€ì“°ê¸° ê·œì¹™ì„ ìƒì„±í•©ë‹ˆë‹¤.
    ë¶„ì„ ë°ì´í„°ê°€ ë§Žì•„ì§ˆìˆ˜ë¡ ë” ì •í™•í•œ ê°€ì´ë“œê°€ ì œê³µë©ë‹ˆë‹¤.

    ë°˜í™˜ê°’:
    - status: 'data_driven' (ë°ì´í„° ê¸°ë°˜) ë˜ëŠ” 'insufficient_data' (ë°ì´í„° ë¶€ì¡±)
    - confidence: ì‹ ë¢°ë„ (0~1, ë¶„ì„ ìƒ˜í”Œ ìˆ˜ì— ë¹„ë¡€)
    - rules: ê¸€ì“°ê¸° ê·œì¹™ (ì œëª©, ë³¸ë¬¸, ì´ë¯¸ì§€ ë“±)
    """
    rules = generate_writing_rules(category)

    return WritingGuideResponse(
        status=rules.get('status', 'insufficient_data'),
        category=rules.get('category', category),
        sample_count=rules.get('sample_count', 0),
        confidence=rules.get('confidence', 0),
        message=rules.get('message'),
        rules=rules.get('rules', {}),
        updated_at=rules.get('updated_at')
    )


@router.get("/writing-guide/markdown")
async def get_writing_guide_markdown(category: str = Query("general", description="ì¹´í…Œê³ ë¦¬")):
    """
    ê¸€ì“°ê¸° ê°€ì´ë“œë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    AI ê¸€ì“°ê¸° í”„ë¡œê·¸ëž¨ì— ì§ì ‘ ë¶™ì—¬ë„£ì„ ìˆ˜ ìžˆëŠ” í˜•ì‹ìž…ë‹ˆë‹¤.
    """
    rules_data = generate_writing_rules(category)
    rules = rules_data.get('rules', {})
    sample_count = rules_data.get('sample_count', 0)
    confidence = rules_data.get('confidence', 0)
    status = rules_data.get('status', 'insufficient_data')

    # ë§ˆí¬ë‹¤ìš´ ìƒì„±
    md = f"""# ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìƒìœ„ë…¸ì¶œ ìµœì í™” ê°€ì´ë“œ

> ì´ ê°€ì´ë“œëŠ” **{sample_count}ê°œ ìƒìœ„ ê¸€ ë¶„ì„** ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìžë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
> ì‹ ë¢°ë„: **{confidence * 100:.0f}%** | ì¹´í…Œê³ ë¦¬: **{category}** | ìƒíƒœ: **{status}**

---

## ðŸ“ ì œëª© ìž‘ì„± ê·œì¹™

"""

    title_rules = rules.get('title', {})
    if title_rules:
        length = title_rules.get('length', {})
        kw_placement = title_rules.get('keyword_placement', {})

        md += f"""```yaml
ê¸€ìž ìˆ˜:
  ìµœì : {length.get('optimal', 30)}ìž
  ë²”ìœ„: {length.get('min', 20)}~{length.get('max', 45)}ìž

í‚¤ì›Œë“œ ë°°ì¹˜:
  í¬í•¨ í•„ìˆ˜: {'ì˜ˆ' if kw_placement.get('include_keyword', True) else 'ì•„ë‹ˆì˜¤'}
  í¬í•¨ë¥ : {kw_placement.get('rate', 85):.1f}%
  ìµœì  ìœ„ì¹˜: {kw_placement.get('best_position', 'front')}
  ìœ„ì¹˜ë³„ ë¶„í¬:
    - ì•žë¶€ë¶„: {kw_placement.get('position_distribution', {}).get('front', 60):.1f}%
    - ì¤‘ê°„: {kw_placement.get('position_distribution', {}).get('middle', 30):.1f}%
    - ë’·ë¶€ë¶„: {kw_placement.get('position_distribution', {}).get('end', 10):.1f}%
```

"""

    md += """## ðŸ“„ ë³¸ë¬¸ ìž‘ì„± ê·œì¹™

"""

    content_rules = rules.get('content', {})
    if content_rules:
        length = content_rules.get('length', {})
        structure = content_rules.get('structure', {})

        md += f"""```yaml
ë³¸ë¬¸ ê¸¸ì´:
  ìµœì : {length.get('optimal', 2000)}ìž
  ìµœì†Œ: {length.get('min', 1500)}ìž
  ìµœëŒ€: {length.get('max', 3500)}ìž

êµ¬ì¡°:
  ì†Œì œëª© ê°œìˆ˜: {structure.get('heading_count', {}).get('min', 3)}~{structure.get('heading_count', {}).get('max', 8)}ê°œ (ê¶Œìž¥: {structure.get('heading_count', {}).get('optimal', 5)}ê°œ)
  í‚¤ì›Œë“œ ë°€ë„: {structure.get('keyword_density', {}).get('min', 0.8)}~{structure.get('keyword_density', {}).get('max', 2.0)} (1000ìžë‹¹)
  í‚¤ì›Œë“œ ë“±ìž¥: {structure.get('keyword_count', {}).get('min', 5)}~{structure.get('keyword_count', {}).get('max', 15)}íšŒ (ê¶Œìž¥: {structure.get('keyword_count', {}).get('optimal', 8)}íšŒ)
```

"""

    md += """## ðŸ–¼ï¸ ì´ë¯¸ì§€/ë™ì˜ìƒ ê·œì¹™

"""

    media_rules = rules.get('media', {})
    if media_rules:
        images = media_rules.get('images', {})
        videos = media_rules.get('videos', {})

        md += f"""```yaml
ì´ë¯¸ì§€:
  ìµœì : {images.get('optimal', 10)}ìž¥
  ë²”ìœ„: {images.get('min', 5)}~{images.get('max', 15)}ìž¥

ë™ì˜ìƒ:
  ì‚¬ìš©ë¥ : {videos.get('usage_rate', 20):.1f}%
  ê¶Œìž¥ ì—¬ë¶€: {'ì˜ˆ' if videos.get('recommended', False) else 'ì„ íƒì‚¬í•­'}
  ê¶Œìž¥ ê°œìˆ˜: {videos.get('optimal', 0)}ê°œ
```

"""

    md += """## âž• ì¶”ê°€ ìš”ì†Œ

"""

    extras = rules.get('extras', {})
    if extras:
        map_info = extras.get('map', {})
        links = extras.get('external_links', {})

        md += f"""```yaml
ì§€ë„:
  ì‚¬ìš©ë¥ : {map_info.get('usage_rate', 15):.1f}%
  ê¶Œìž¥: {'ì˜ˆ (ì§€ì—­ í‚¤ì›Œë“œì˜ ê²½ìš°)' if map_info.get('recommended', False) else 'ì„ íƒì‚¬í•­'}

ì™¸ë¶€ ë§í¬:
  ì‚¬ìš©ë¥ : {links.get('usage_rate', 25):.1f}%
  ê¶Œìž¥: {'ì˜ˆ' if links.get('recommended', False) else 'ì„ íƒì‚¬í•­'}
```

"""

    md += f"""---

## ðŸ’¡ ì ìš© ë°©ë²•

ì´ ê°€ì´ë“œë¥¼ AI ê¸€ì“°ê¸° í”„ë¡œê·¸ëž¨(ChatGPT, Claude ë“±)ì— í•¨ê»˜ ìž…ë ¥í•˜ì„¸ìš”:

```
[ì´ ê°€ì´ë“œ ì „ì²´ ë³µì‚¬] + "í‚¤ì›Œë“œ: OOOë¡œ ë¸”ë¡œê·¸ ê¸€ ìž‘ì„±í•´ì¤˜"
```

---

*ì´ ê°€ì´ë“œëŠ” {sample_count}ê°œì˜ ìƒìœ„ ê¸€ ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
*ë” ë§Žì€ í‚¤ì›Œë“œë¥¼ ê²€ìƒ‰í• ìˆ˜ë¡ ê°€ì´ë“œ ì •í™•ë„ê°€ ë†’ì•„ì§‘ë‹ˆë‹¤.*
*ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {rules_data.get('updated_at', datetime.now().isoformat())}*
"""

    return {"content": md, "category": category, "sample_count": sample_count}


@router.get("/stats", response_model=AnalysisStatsResponse)
async def get_analysis_stats():
    """
    ë¶„ì„ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    - ì´ ë¶„ì„ ìˆ˜
    - ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ ìˆ˜
    - ìµœê·¼ ë¶„ì„ ëª©ë¡
    """
    total = get_analysis_count()
    category_breakdown = get_category_stats()
    recent = get_recent_analyses(limit=20)

    return AnalysisStatsResponse(
        total_analyses=total,
        category_breakdown=category_breakdown,
        recent_analyses=recent
    )


@router.get("/all-patterns")
async def get_all_category_patterns():
    """
    ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ íŒ¨í„´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    patterns = get_all_patterns()
    return {
        "patterns": patterns,
        "total_categories": len(patterns)
    }


@router.post("/refresh-patterns")
async def refresh_all_patterns(background_tasks: BackgroundTasks):
    """
    ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ íŒ¨í„´ì„ ìž¬ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    categories = ['general', 'hospital', 'restaurant', 'beauty', 'parenting', 'travel', 'tech']

    for category in categories:
        background_tasks.add_task(update_aggregated_patterns, category)

    return {
        "message": f"{len(categories)}ê°œ ì¹´í…Œê³ ë¦¬ íŒ¨í„´ ì—…ë°ì´íŠ¸ ì‹œìž‘",
        "categories": categories
    }


# ===== ìžë™ ë¶„ì„ ì—°ë™ í•¨ìˆ˜ (í‚¤ì›Œë“œ ê²€ìƒ‰ì—ì„œ í˜¸ì¶œ) =====

async def auto_analyze_top_posts(keyword: str, search_results: List[Dict], top_n: int = 3):
    """
    í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ìƒìœ„ ê¸€ë“¤ì„ ìžë™ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
    blogs.pyì˜ search_keyword_with_tabsì—ì„œ í˜¸ì¶œë©ë‹ˆë‹¤.
    """
    if not search_results:
        return

    category = detect_category(keyword)
    analyzed = 0

    for item in search_results[:top_n]:
        try:
            post_analysis = await analyze_post(item['post_url'], keyword)

            analysis_data = {
                'keyword': keyword,
                'rank': item['rank'],
                'blog_id': item['blog_id'],
                'post_url': item['post_url'],
                'title_length': len(item.get('post_title', '')),
                'title_has_keyword': post_analysis.get('title_has_keyword', False),
                'title_keyword_position': post_analysis.get('title_keyword_position', -1),
                'content_length': post_analysis.get('content_length', 0),
                'image_count': post_analysis.get('image_count', 0),
                'video_count': post_analysis.get('video_count', 0),
                'heading_count': post_analysis.get('heading_count', 0),
                'paragraph_count': post_analysis.get('paragraph_count', 0),
                'keyword_count': post_analysis.get('keyword_count', 0),
                'keyword_density': post_analysis.get('keyword_density', 0),
                'has_map': post_analysis.get('has_map', False),
                'has_link': post_analysis.get('has_link', False),
                'like_count': post_analysis.get('like_count', 0),
                'comment_count': post_analysis.get('comment_count', 0),
                'post_age_days': post_analysis.get('post_age_days'),
                'category': category,
                'data_quality': 'high' if post_analysis.get('data_fetched') else 'low'
            }

            save_post_analysis(analysis_data)
            analyzed += 1

        except Exception as e:
            logger.warning(f"Auto-analysis failed for {item.get('post_url')}: {e}")

    # íŒ¨í„´ ì—…ë°ì´íŠ¸
    if analyzed > 0:
        update_aggregated_patterns(category)
        update_aggregated_patterns('general')

    logger.info(f"Auto-analyzed {analyzed} top posts for '{keyword}' (category: {category})")
    return analyzed
