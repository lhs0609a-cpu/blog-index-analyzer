"""
í‚¤ì›Œë“œ ë¶„ì„ ì‹œìŠ¤í…œ - í•µì‹¬ ì„œë¹„ìŠ¤
"""
import re
import asyncio
import logging
from typing import List, Dict, Any, Optional, Tuple
from collections import defaultdict

import httpx

from models.keyword_analysis import (
    KeywordType, CompetitionLevel, EntryDifficulty,
    KeywordData, TabRatio, Top10Stats, CompetitionAnalysis,
    KeywordHierarchy, SubKeyword, ClassifiedKeyword,
    KeywordAnalysisResponse
)
from database.keyword_analysis_db import (
    get_cached_analysis, cache_analysis,
    get_learned_type, save_keyword_type,
    get_sub_keywords, save_keyword_hierarchy,
    save_competition_history, get_cached_tab_ratio, cache_tab_ratio
)
from config import settings

logger = logging.getLogger(__name__)


class KeywordClassifier:
    """í‚¤ì›Œë“œ ìœ í˜• ë¶„ë¥˜ê¸° (ê·œì¹™ ê¸°ë°˜ + í•™ìŠµ)"""

    # ì˜ë£Œ/ë³‘ì› ê´€ë ¨ í‚¤ì›Œë“œ (ê²½ìŸ íŠ¹ìˆ˜ì„± ë°˜ì˜)
    MEDICAL_KEYWORDS = [
        # í”¼ë¶€ê³¼ ê´€ë ¨
        'ì—¬ë“œë¦„', 'í‰í„°', 'ëª¨ê³µ', 'ê¸°ë¯¸', 'ì£¼ê·¼ê¹¨', 'ì¡í‹°', 'í”¼ë¶€', 'ë ˆì´ì €', 'í•„ë§',
        'ë³´í†¡ìŠ¤', 'í•„ëŸ¬', 'ë¦¬í”„íŒ…', 'íƒˆëª¨', 'ë‘í”¼', 'ì•„í† í”¼', 'ê±´ì„ ', 'ìŠµì§„',
        # ì„±í˜•ì™¸ê³¼ ê´€ë ¨
        'ì½”ì„±í˜•', 'ëˆˆì„±í˜•', 'ìŒêº¼í’€', 'ì§€ë°©í¡ì…', 'ìœ¤ê³½', 'ì•ˆë©´ìœ¤ê³½', 'ì–‘ì•…', 'í„±',
        # ì¹˜ê³¼ ê´€ë ¨
        'ì„í”Œë€íŠ¸', 'ì¹˜ì•„êµì •', 'êµì •', 'ë¼ë¯¸ë„¤ì´íŠ¸', 'ì¹˜ì•„ë¯¸ë°±', 'ì‡ëª¸', 'ì‚¬ë‘ë‹ˆ',
        # í•œì˜ì› ê´€ë ¨
        'í•œì˜ì›', 'í•œë°©', 'ì¹¨', 'ì¶”ë‚˜', 'í•œì•½', 'ë‹¤ì´ì–´íŠ¸í•œì˜ì›', 'ë¹„ë§Œí•œì˜ì›',
        # ì •í˜•ì™¸ê³¼/í†µì¦ì˜í•™ê³¼
        'í—ˆë¦¬', 'ë””ìŠ¤í¬', 'ì²™ì¶”', 'ê´€ì ˆ', 'ë¬´ë¦', 'ì–´ê¹¨', 'ëª©', 'í†µì¦',
        # ì•ˆê³¼
        'ë¼ì‹', 'ë¼ì„¹', 'ìŠ¤ë§ˆì¼ë¼ì‹', 'ë°±ë‚´ì¥', 'ë…¹ë‚´ì¥', 'ì•ˆê²½', 'ì‹œë ¥êµì •',
        # ê¸°íƒ€ ì „ë¬¸ê³¼
        'ë¹„ì—¼', 'ì¶•ë†ì¦', 'ì´ë¹„ì¸í›„ê³¼', 'ë‚´ì‹œê²½', 'ê±´ê°•ê²€ì§„', 'ì‚°ë¶€ì¸ê³¼', 'ë¹„ë‡¨ê¸°ê³¼',
    ]

    # ë³‘ì› ê³µì‹ ë¸”ë¡œê·¸ íŒ¨í„´
    OFFICIAL_BLOG_PATTERNS = [
        r'.*ë³‘ì›$', r'.*ì˜ì›$', r'.*í´ë¦¬ë‹‰$', r'.*ì„¼í„°$', r'.*í•œì˜ì›$',
        r'.*ì¹˜ê³¼$', r'.*í”¼ë¶€ê³¼$', r'.*ì„±í˜•ì™¸ê³¼$', r'.*ì•ˆê³¼$', r'.*ì´ë¹„ì¸í›„ê³¼$',
    ]

    # í‚¤ì›Œë“œ ìœ í˜•ë³„ íŒ¨í„´
    TYPE_PATTERNS = {
        KeywordType.INFO: {
            'suffixes': ['ë€', 'ì´ë€', 'ëœ»', 'ì˜ë¯¸', 'ì •ì˜'],
            'keywords': ['ì›ì¸', 'ì¦ìƒ', 'íš¨ê³¼', 'ë°©ë²•', 'ì˜ˆë°©', 'ì¹˜ë£Œë²•', 'ê´€ë¦¬', 'ê°œì„ ',
                        'ì•Œì•„ë³´ê¸°', 'ì•Œì•„ë³´', 'ì •ë³´', 'ì„¤ëª…', 'ì´í•´', 'íŠ¹ì§•'],
            'patterns': [r'.*ì´ë€\??$', r'.*ë€\??$', r'.*ëœ»$', r'ì–´ë–»ê²Œ.*']
        },
        KeywordType.SYMPTOM: {
            'keywords': ['ì•„í”„', 'í†µì¦', 'ì‘¤ì‹œ', 'ì €ë¦¼', 'ì–´ì§€ëŸ¬', 'ë‘í†µ', 'ë³µí†µ', 'ìš”í†µ',
                        'ë¶“ê¸°', 'ë¶€ì¢…', 'ê°€ë ¤', 'ë”°ê°€', 'ì“°ë¼', 'ë»ê·¼', 'ê²°ë¦¼', 'ë»£ë»£',
                        'í”¼ê³¤', 'ë¬´ê¸°ë ¥', 'ì‹ì€ë•€', 'ì—´ê°', 'ì˜¤í•œ'],
            'patterns': [r'.*ì•„í”„.*', r'.*í†µì¦.*', r'.*ì¦ìƒ.*']
        },
        KeywordType.HOSPITAL: {
            'keywords': ['ë³‘ì›', 'ì˜ì›', 'í´ë¦¬ë‹‰', 'ì„¼í„°', 'í•œì˜ì›', 'ì¹˜ê³¼', 'ì•ˆê³¼', 'í”¼ë¶€ê³¼',
                        'ì¶”ì²œ', 'ì˜í•˜ëŠ”', 'ìœ ëª…í•œ', 'ì¢‹ì€', 'ë§›ì§‘', 'ëª…ì˜', 'ì „ë¬¸'],
            'patterns': [r'.*ë³‘ì›\s*ì¶”ì²œ.*', r'.*ì˜\s*í•˜ëŠ”.*ë³‘ì›.*', r'.*ì–´ë””.*']
        },
        KeywordType.COST: {
            'keywords': ['ë¹„ìš©', 'ê°€ê²©', 'ì–¼ë§ˆ', 'ê²€ì‚¬ë¹„', 'ì¹˜ë£Œë¹„', 'ìˆ˜ìˆ ë¹„', 'ì‹œìˆ ë¹„',
                        'ë³´í—˜', 'ì‹¤ë¹„', 'ê¸‰ì—¬', 'ë¹„ê¸‰ì—¬', 'ë¬´ë£Œ', 'í• ì¸', 'ì´ë²¤íŠ¸'],
            'patterns': [r'.*ì–¼ë§ˆ.*', r'.*ë¹„ìš©.*', r'.*ê°€ê²©.*']
        },
        KeywordType.LOCAL: {
            # ì£¼ìš” ì§€ì—­ (êµ¬ ë‹¨ìœ„)
            'prefixes': ['ê°•ë‚¨', 'ì„œì´ˆ', 'ì†¡íŒŒ', 'ê°•ë™', 'ê°•ì„œ', 'ë§ˆí¬', 'ì˜ë“±í¬', 'ìš©ì‚°',
                        'ì„±ë¶', 'ë…¸ì›', 'ë¶„ë‹¹', 'íŒêµ', 'ì¼ì‚°', 'ìˆ˜ì›', 'ì•ˆì–‘', 'ë¶€ì²œ',
                        'ì¸ì²œ', 'ì˜ì •ë¶€', 'ëŒ€ì „', 'ëŒ€êµ¬', 'ë¶€ì‚°', 'ê´‘ì£¼', 'ìš¸ì‚°',
                        # ì£¼ìš” ìƒê¶Œ/ì—­ì„¸ê¶Œ ì¶”ê°€
                        'í™ëŒ€', 'ì‹ ì´Œ', 'ì´íƒœì›', 'ì••êµ¬ì •', 'ì²­ë‹´', 'ì ì‹¤', 'ê±´ëŒ€', 'ì™•ì‹­ë¦¬',
                        'ì‹ ë¦¼', 'ì‚¬ë‹¹', 'êµëŒ€', 'ì—­ì‚¼', 'ì„ ë¦‰', 'ì‚¼ì„±', 'ì ì‹¤ìƒˆë‚´',
                        'ëª©ë™', 'ì—¬ì˜ë„', 'í•©ì •', 'ìƒìˆ˜', 'ì—°ë‚¨ë™', 'ë§ì›', 'ì„±ìˆ˜',
                        'í•´ìš´ëŒ€', 'ì„œë©´', 'ë™ë˜', 'ë‚¨í¬ë™', 'ê´‘ì•ˆë¦¬',
                        'ë™ì„±ë¡œ', 'ìˆ˜ì„±êµ¬', 'ë‹¬ì„œêµ¬',
                        'ìœ ì„±', 'ë‘”ì‚°', 'ëŒ€ë•',
                        'ìƒë¬´ì§€êµ¬', 'ì²¨ë‹¨', 'ìˆ˜ì™„',
                        # ê²½ê¸°ë„ ì£¼ìš” ì§€ì—­
                        'ìœ„ë¡€', 'ë™íƒ„', 'ê´‘êµ', 'ì˜í†µ', 'ì •ì', 'ë¯¸ê¸ˆ', 'ìˆ˜ì§€', 'ì£½ì „',
                        'í‰ì´Œ', 'ë²”ê³„', 'ì‚°ë³¸', 'ê¸ˆì •', 'í‰ë‚´í˜¸í‰', 'ë³„ë‚´',
                        'ê¹€í¬', 'íŒŒì£¼', 'ê³ ì–‘', 'í™”ì •', 'í–‰ì‹ '],
            'patterns': [r'^(ê°•ë‚¨|ì„œì´ˆ|ë¶„ë‹¹|íŒêµ|ì¼ì‚°|í™ëŒ€|ì‹ ì´Œ|ì••êµ¬ì •|ì²­ë‹´|ì ì‹¤|ê±´ëŒ€).*ë³‘ì›.*',
                        r'^(ê°•ë‚¨|ì„œì´ˆ|ë¶„ë‹¹|íŒêµ|ì¼ì‚°|í™ëŒ€|ì‹ ì´Œ|ì••êµ¬ì •|ì²­ë‹´|ì ì‹¤|ê±´ëŒ€).*ì˜ì›.*',
                        r'^(ê°•ë‚¨|ì„œì´ˆ|ë¶„ë‹¹|íŒêµ|ì¼ì‚°|í™ëŒ€|ì‹ ì´Œ|ì••êµ¬ì •|ì²­ë‹´|ì ì‹¤|ê±´ëŒ€).*í•œì˜ì›.*',
                        r'^(ê°•ë‚¨|ì„œì´ˆ|ë¶„ë‹¹|íŒêµ|ì¼ì‚°|í™ëŒ€|ì‹ ì´Œ|ì••êµ¬ì •|ì²­ë‹´|ì ì‹¤|ê±´ëŒ€).*í´ë¦¬ë‹‰.*',
                        r'^(ê°•ë‚¨|ì„œì´ˆ|ë¶„ë‹¹|íŒêµ|ì¼ì‚°|í™ëŒ€|ì‹ ì´Œ|ì••êµ¬ì •|ì²­ë‹´|ì ì‹¤|ê±´ëŒ€).*í”¼ë¶€ê³¼.*',
                        r'.*ì—­\s*(ë³‘ì›|ì˜ì›|í•œì˜ì›).*',
                        r'.*ë™\s*(ë³‘ì›|ì˜ì›).*',
                        r'.*êµ¬\s*(ë³‘ì›|ì˜ì›).*']
        },
        KeywordType.BROAD: {
            'prefixes': ['ì„œìš¸', 'ê²½ê¸°', 'ì¸ì²œ', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ëŒ€ì „', 'ê´‘ì£¼', 'ìš¸ì‚°', 'ì„¸ì¢…',
                        'ê°•ì›', 'ì¶©ë¶', 'ì¶©ë‚¨', 'ì „ë¶', 'ì „ë‚¨', 'ê²½ë¶', 'ê²½ë‚¨', 'ì œì£¼'],
            'patterns': [r'^(ì„œìš¸|ë¶€ì‚°|ëŒ€êµ¬|ì¸ì²œ|ê´‘ì£¼|ëŒ€ì „|ìš¸ì‚°).*ë³‘ì›.*']
        }
    }

    def __init__(self):
        self._compile_patterns()

    def _compile_patterns(self):
        """ì •ê·œì‹ íŒ¨í„´ ì‚¬ì „ ì»´íŒŒì¼"""
        self._compiled_patterns = {}
        for kw_type, rules in self.TYPE_PATTERNS.items():
            if 'patterns' in rules:
                self._compiled_patterns[kw_type] = [
                    re.compile(p, re.IGNORECASE) for p in rules['patterns']
                ]

    def classify(self, keyword: str) -> Tuple[KeywordType, float]:
        """
        í‚¤ì›Œë“œ ë¶„ë¥˜
        Returns: (í‚¤ì›Œë“œ ìœ í˜•, ì‹ ë¢°ë„)
        """
        keyword_lower = keyword.lower().strip()

        # 1. í•™ìŠµëœ ë¶„ë¥˜ í™•ì¸
        learned = get_learned_type(keyword)
        if learned and learned.get('is_verified'):
            return (KeywordType(learned['classified_type']), learned['confidence'])

        # 2. ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜ (ìš°ì„ ìˆœìœ„: ì§€ì—­ > ê´‘ì—­ > ë¹„ìš© > ë³‘ì› > ì¦ìƒ > ì •ë³´)
        # ì§€ì—­í˜• ë¨¼ì € ì²´í¬ (ê°€ì¥ êµ¬ì²´ì )
        result = self._check_local_or_broad(keyword_lower)
        if result:
            return result

        # ë¹„ìš©/ê²€ì‚¬í˜•
        result = self._check_type(keyword_lower, KeywordType.COST)
        if result:
            return result

        # ë³‘ì›íƒìƒ‰í˜•
        result = self._check_type(keyword_lower, KeywordType.HOSPITAL)
        if result:
            return result

        # ì¦ìƒí˜•
        result = self._check_type(keyword_lower, KeywordType.SYMPTOM)
        if result:
            return result

        # ì •ë³´í˜•
        result = self._check_type(keyword_lower, KeywordType.INFO)
        if result:
            return result

        # í•™ìŠµëœ ë¶„ë¥˜ (ë¯¸ê²€ì¦ë„ ì‚¬ìš©)
        if learned:
            return (KeywordType(learned['classified_type']), learned['confidence'])

        # ê¸°ë³¸ê°’: ì •ë³´í˜•
        return (KeywordType.INFO, 0.3)

    def _check_local_or_broad(self, keyword: str) -> Optional[Tuple[KeywordType, float]]:
        """ì§€ì—­í˜•/ê´‘ì—­í˜• ì²´í¬"""
        local_rules = self.TYPE_PATTERNS[KeywordType.LOCAL]
        broad_rules = self.TYPE_PATTERNS[KeywordType.BROAD]

        # ì§€ì—­í˜• ì²´í¬ (ë” êµ¬ì²´ì ì¸ ì§€ì—­)
        for prefix in local_rules.get('prefixes', []):
            if keyword.startswith(prefix):
                # ë³‘ì›/ì˜ì› ê´€ë ¨ í‚¤ì›Œë“œì¸ì§€ í™•ì¸
                if any(h in keyword for h in ['ë³‘ì›', 'ì˜ì›', 'í´ë¦¬ë‹‰', 'ì„¼í„°', 'í•œì˜ì›']):
                    return (KeywordType.LOCAL, 0.9)

        # ì •ê·œì‹ íŒ¨í„´ ì²´í¬
        for pattern in self._compiled_patterns.get(KeywordType.LOCAL, []):
            if pattern.match(keyword):
                return (KeywordType.LOCAL, 0.85)

        # ê´‘ì—­í˜• ì²´í¬
        for prefix in broad_rules.get('prefixes', []):
            if keyword.startswith(prefix):
                if any(h in keyword for h in ['ë³‘ì›', 'ì˜ì›', 'í´ë¦¬ë‹‰', 'ì„¼í„°']):
                    return (KeywordType.BROAD, 0.85)

        for pattern in self._compiled_patterns.get(KeywordType.BROAD, []):
            if pattern.match(keyword):
                return (KeywordType.BROAD, 0.8)

        return None

    def _check_type(self, keyword: str, kw_type: KeywordType) -> Optional[Tuple[KeywordType, float]]:
        """íŠ¹ì • ìœ í˜• ì²´í¬"""
        rules = self.TYPE_PATTERNS.get(kw_type, {})

        # ì ‘ë¯¸ì‚¬ ì²´í¬
        for suffix in rules.get('suffixes', []):
            if keyword.endswith(suffix):
                return (kw_type, 0.9)

        # ì ‘ë‘ì‚¬ ì²´í¬
        for prefix in rules.get('prefixes', []):
            if keyword.startswith(prefix):
                return (kw_type, 0.85)

        # í‚¤ì›Œë“œ í¬í•¨ ì²´í¬
        for kw in rules.get('keywords', []):
            if kw in keyword:
                return (kw_type, 0.8)

        # ì •ê·œì‹ íŒ¨í„´ ì²´í¬
        for pattern in self._compiled_patterns.get(kw_type, []):
            if pattern.match(keyword):
                return (kw_type, 0.75)

        return None

    def classify_batch(self, keywords: List[str]) -> List[ClassifiedKeyword]:
        """ë°°ì¹˜ ë¶„ë¥˜"""
        results = []
        for kw in keywords:
            kw_type, confidence = self.classify(kw)
            results.append(ClassifiedKeyword(
                keyword=kw,
                keyword_type=kw_type,
                confidence=confidence
            ))
            # í•™ìŠµ ì €ì¥
            save_keyword_type(kw, kw_type.value, confidence)
        return results

    def is_medical_keyword(self, keyword: str) -> Tuple[bool, List[str]]:
        """
        ì˜ë£Œ/ë³‘ì› ê´€ë ¨ í‚¤ì›Œë“œì¸ì§€ í™•ì¸
        Returns: (ì˜ë£Œ í‚¤ì›Œë“œ ì—¬ë¶€, ë§¤ì¹­ëœ ì˜ë£Œ í‚¤ì›Œë“œ ëª©ë¡)
        """
        keyword_lower = keyword.lower()
        matched = []
        for medical_kw in self.MEDICAL_KEYWORDS:
            if medical_kw in keyword_lower:
                matched.append(medical_kw)
        return (len(matched) > 0, matched)

    def is_local_medical_keyword(self, keyword: str) -> bool:
        """ì§€ì—­ + ì˜ë£Œ ë³µí•© í‚¤ì›Œë“œì¸ì§€ í™•ì¸ (ê²½ìŸ ë§¤ìš° ì–´ë ¤ì›€)"""
        kw_type, _ = self.classify(keyword)
        is_local = kw_type in [KeywordType.LOCAL, KeywordType.BROAD]
        is_medical, _ = self.is_medical_keyword(keyword)
        return is_local and is_medical

    @staticmethod
    def is_official_blog(blog_name: str) -> bool:
        """ë³‘ì›/ì˜ì› ê³µì‹ ë¸”ë¡œê·¸ì¸ì§€ í™•ì¸"""
        if not blog_name:
            return False
        blog_name = blog_name.strip()
        official_suffixes = ['ë³‘ì›', 'ì˜ì›', 'í´ë¦¬ë‹‰', 'ì„¼í„°', 'í•œì˜ì›',
                           'ì¹˜ê³¼', 'í”¼ë¶€ê³¼', 'ì„±í˜•ì™¸ê³¼', 'ì•ˆê³¼', 'ì´ë¹„ì¸í›„ê³¼',
                           'ì •í˜•ì™¸ê³¼', 'ë‚´ê³¼', 'ì™¸ê³¼', 'ì‚°ë¶€ì¸ê³¼', 'ë¹„ë‡¨ê¸°ê³¼']
        for suffix in official_suffixes:
            if blog_name.endswith(suffix):
                return True
        return False


class KeywordAnalysisService:
    """í‚¤ì›Œë“œ ì¢…í•© ë¶„ì„ ì„œë¹„ìŠ¤"""

    def __init__(self):
        self.classifier = KeywordClassifier()
        self.http_client = None

    async def _get_client(self) -> httpx.AsyncClient:
        """HTTP í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
        if self.http_client is None:
            self.http_client = httpx.AsyncClient(timeout=30.0)
        return self.http_client

    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.http_client:
            await self.http_client.aclose()
            self.http_client = None

    async def analyze_keyword(
        self,
        keyword: str,
        expand_related: bool = True,
        min_search_volume: int = 100,
        max_keywords: int = 50,
        my_blog_id: Optional[str] = None
    ) -> KeywordAnalysisResponse:
        """
        í‚¤ì›Œë“œ ì¢…í•© ë¶„ì„

        1. ìºì‹œ í™•ì¸
        2. ë„¤ì´ë²„ ê´‘ê³  APIë¡œ ì—°ê´€ í‚¤ì›Œë“œ ì¡°íšŒ
        3. í‚¤ì›Œë“œ í™•ì¥ (expand_related=Trueì¼ ë•Œ)
        4. í•„í„°ë§ (ê²€ìƒ‰ëŸ‰, ì¤‘ë³µ)
        5. ìœ í˜• ë¶„ë¥˜
        6. ê²½ìŸë„ ë¶„ì„
        """
        try:
            # 1. ìºì‹œ í™•ì¸
            cached = get_cached_analysis(keyword)
            if cached:
                logger.info(f"Cache hit for keyword: {keyword}")
                response = KeywordAnalysisResponse(**cached)
                response.cached = True
                return response

            # 2. ì—°ê´€ í‚¤ì›Œë“œ ì¡°íšŒ
            related_keywords = await self._fetch_related_keywords(keyword)

            # 3. ê¸°ì¡´ DBì—ì„œ ì„¸ë¶€ í‚¤ì›Œë“œ ì¡°íšŒ ë° API í•™ìŠµ
            if expand_related:
                expanded = await self._expand_keywords(keyword, related_keywords)
                related_keywords.extend(expanded)

            # 4. í•„í„°ë§
            filtered = self._filter_keywords(related_keywords, min_search_volume)

            # 5. ì¤‘ë³µ ì œê±°
            unique_keywords = self._remove_duplicates(filtered)

            # 6. ìœ í˜• ë¶„ë¥˜
            classified = []
            for kw in unique_keywords[:max_keywords]:
                kw_type, confidence = self.classifier.classify(kw.keyword)
                kw.keyword_type = kw_type
                kw.confidence = confidence
                classified.append(kw)

            # 7. ê²½ìŸë„ ë¶„ì„
            competition = await self._analyze_competition(keyword, my_blog_id)

            # 8. ìœ í˜•ë³„ ë¶„í¬ ê³„ì‚°
            type_dist = self._calculate_type_distribution(classified)

            # 9. ì¶”ì²œ ë©”ì‹œì§€ ìƒì„±
            recommendations = self._generate_recommendations(competition, type_dist)

            response = KeywordAnalysisResponse(
                success=True,
                main_keyword=keyword,
                keywords=classified,
                total_count=len(related_keywords),
                filtered_count=len(classified),
                competition_summary=competition,
                type_distribution=type_dist,
                recommendations=recommendations
            )

            # ìºì‹œ ì €ì¥
            cache_analysis(keyword, response.model_dump())

            return response

        except Exception as e:
            logger.error(f"Error analyzing keyword '{keyword}': {e}")
            return KeywordAnalysisResponse(
                success=False,
                main_keyword=keyword,
                error=str(e)
            )

    async def _fetch_related_keywords(self, keyword: str) -> List[KeywordData]:
        """ë„¤ì´ë²„ ê´‘ê³  APIì—ì„œ ì—°ê´€ í‚¤ì›Œë“œ ì¡°íšŒ"""
        # ê¸°ì¡´ blogs.pyì˜ get_related_keywords_from_searchad ë¡œì§ í™œìš©
        from routers.blogs import get_related_keywords_from_searchad

        try:
            result = await get_related_keywords_from_searchad(keyword)

            keywords = []
            for kw in result.keywords:
                keywords.append(KeywordData(
                    keyword=kw.keyword,
                    monthly_pc_search=kw.monthly_pc_search or 0,
                    monthly_mobile_search=kw.monthly_mobile_search or 0,
                    monthly_total_search=kw.monthly_total_search or 0,
                    competition=kw.competition or "ë‚®ìŒ",
                    competition_index=self._competition_to_index(kw.competition)
                ))

            return keywords

        except Exception as e:
            logger.error(f"Error fetching related keywords: {e}")
            return []

    def _competition_to_index(self, competition: str) -> float:
        """ê²½ìŸë„ ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜"""
        mapping = {
            "ë†’ìŒ": 0.9,
            "ì¤‘ê°„": 0.5,
            "ë‚®ìŒ": 0.2
        }
        return mapping.get(competition, 0.3)

    async def _expand_keywords(
        self,
        main_keyword: str,
        existing_keywords: List[KeywordData]
    ) -> List[KeywordData]:
        """
        í‚¤ì›Œë“œ í™•ì¥ (API í•™ìŠµ ë°©ì‹)
        - ê¸°ì¡´ DBì—ì„œ ì €ì¥ëœ ì„¸ë¶€ í‚¤ì›Œë“œ ì¡°íšŒ
        - ìƒˆë¡œìš´ ì—°ê´€ í‚¤ì›Œë“œëŠ” DBì— ì €ì¥ (í•™ìŠµ)
        """
        expanded = []
        existing_set = {kw.keyword for kw in existing_keywords}

        # DBì—ì„œ ê¸°ì¡´ ì„¸ë¶€ í‚¤ì›Œë“œ ì¡°íšŒ
        sub_keywords = get_sub_keywords(main_keyword)
        for sub in sub_keywords:
            if sub['sub_keyword'] not in existing_set:
                expanded.append(KeywordData(
                    keyword=sub['sub_keyword'],
                    monthly_total_search=sub.get('search_volume', 0),
                    keyword_type=KeywordType(sub.get('keyword_type', 'ë¯¸ë¶„ë¥˜')) if sub.get('keyword_type') else KeywordType.UNKNOWN
                ))

        # ìƒˆë¡œìš´ ì—°ê´€ í‚¤ì›Œë“œë¥¼ DBì— ì €ì¥ (í•™ìŠµ)
        for kw in existing_keywords:
            if kw.keyword != main_keyword:
                kw_type, _ = self.classifier.classify(kw.keyword)
                save_keyword_hierarchy(
                    main_keyword=main_keyword,
                    sub_keyword=kw.keyword,
                    search_volume=kw.monthly_total_search,
                    keyword_type=kw_type.value
                )

        return expanded

    def _filter_keywords(
        self,
        keywords: List[KeywordData],
        min_search_volume: int
    ) -> List[KeywordData]:
        """ê²€ìƒ‰ëŸ‰ ê¸°ì¤€ í•„í„°ë§"""
        return [
            kw for kw in keywords
            if kw.monthly_total_search >= min_search_volume
        ]

    def _remove_duplicates(self, keywords: List[KeywordData]) -> List[KeywordData]:
        """ì¤‘ë³µ í‚¤ì›Œë“œ ì œê±°"""
        seen = set()
        unique = []
        for kw in keywords:
            # ê³µë°± ì œê±° í›„ ë¹„êµ
            normalized = kw.keyword.replace(" ", "").lower()
            if normalized not in seen:
                seen.add(normalized)
                unique.append(kw)
        return unique

    async def _analyze_competition(
        self,
        keyword: str,
        my_blog_id: Optional[str] = None
    ) -> CompetitionAnalysis:
        """ê²½ìŸë„ ë¶„ì„ (ì˜ë£Œ/ì§€ì—­ í‚¤ì›Œë“œ íŠ¹ìˆ˜ì„± ë°˜ì˜)"""
        # ìƒìœ„ ë¸”ë¡œê·¸ ë¶„ì„ (ê¸°ì¡´ search-keyword-with-tabs í™œìš©)
        from routers.blogs import search_keyword_with_tabs

        try:
            search_result = await search_keyword_with_tabs(keyword, limit=10, analyze_content=True)

            # ìƒìœ„ 10ê°œ ë¸”ë¡œê·¸ í†µê³„
            top10_scores = []
            top10_c_ranks = []
            top10_dias = []
            top10_posts = []
            top10_visitors = []
            official_blog_count = 0  # ë³‘ì› ê³µì‹ ë¸”ë¡œê·¸ ìˆ˜

            for blog in search_result.results[:10]:
                if blog.index:
                    top10_scores.append(blog.index.total_score)
                    if blog.index.score_breakdown:
                        top10_c_ranks.append(blog.index.score_breakdown.get('c_rank', 0))
                        top10_dias.append(blog.index.score_breakdown.get('dia', 0))
                if blog.stats:
                    top10_posts.append(blog.stats.total_posts)
                    top10_visitors.append(blog.stats.total_visitors)

                # ë³‘ì› ê³µì‹ ë¸”ë¡œê·¸ ì²´í¬
                blog_name = getattr(blog, 'blog_name', '') or getattr(blog, 'blogger_name', '') or ''
                if self.classifier.is_official_blog(blog_name):
                    official_blog_count += 1

            # í†µê³„ ê³„ì‚°
            top10_stats = Top10Stats(
                avg_total_score=sum(top10_scores) / len(top10_scores) if top10_scores else 0,
                avg_c_rank=sum(top10_c_ranks) / len(top10_c_ranks) if top10_c_ranks else 0,
                avg_dia=sum(top10_dias) / len(top10_dias) if top10_dias else 0,
                min_score=min(top10_scores) if top10_scores else 0,
                max_score=max(top10_scores) if top10_scores else 0,
                avg_posts=int(sum(top10_posts) / len(top10_posts)) if top10_posts else 0,
                avg_visitors=int(sum(top10_visitors) / len(top10_visitors)) if top10_visitors else 0
            )

            # íƒ­ë³„ ë¹„ìœ¨ ì¡°íšŒ
            tab_ratio = await self.get_tab_ratio(keyword)

            # ì˜ë£Œ/ì§€ì—­ í‚¤ì›Œë“œ íŠ¹ìˆ˜ì„± ì²´í¬
            is_local_medical = self.classifier.is_local_medical_keyword(keyword)
            is_medical, medical_matches = self.classifier.is_medical_keyword(keyword)
            official_blog_ratio = official_blog_count / 10 if len(search_result.results) >= 10 else official_blog_count / max(len(search_result.results), 1)

            # ê²½ìŸë„ ë ˆë²¨ ê²°ì • (ì˜ë£Œ/ì§€ì—­ í‚¤ì›Œë“œëŠ” ìƒí–¥ ì¡°ì •)
            avg_score = top10_stats.avg_total_score

            # ê¸°ë³¸ ê²½ìŸë„ íŒë‹¨
            if avg_score >= 75:
                competition_level = CompetitionLevel.HIGH
            elif avg_score >= 55:
                competition_level = CompetitionLevel.MEDIUM
            else:
                competition_level = CompetitionLevel.LOW

            # ì§€ì—­+ì˜ë£Œ í‚¤ì›Œë“œì´ê±°ë‚˜ ë³‘ì› ê³µì‹ ë¸”ë¡œê·¸ê°€ ë§ìœ¼ë©´ ê²½ìŸë„ ìƒí–¥
            if is_local_medical or official_blog_ratio >= 0.5:
                competition_level = CompetitionLevel.HIGH

            # ì§„ì… ë‚œì´ë„ ê²°ì • (ì˜ë£Œ/ì§€ì—­ í‚¤ì›Œë“œ íŠ¹ìˆ˜ì„± ë°˜ì˜)
            if is_local_medical:
                # ì§€ì—­+ì˜ë£Œ í‚¤ì›Œë“œëŠ” ë¬´ì¡°ê±´ ì–´ë ¤ì›€ ì´ìƒ
                if official_blog_ratio >= 0.7:
                    entry_difficulty = EntryDifficulty.VERY_HARD
                elif official_blog_ratio >= 0.5:
                    entry_difficulty = EntryDifficulty.VERY_HARD
                else:
                    entry_difficulty = EntryDifficulty.HARD
            elif is_medical and official_blog_ratio >= 0.5:
                # ì˜ë£Œ í‚¤ì›Œë“œ + ê³µì‹ ë¸”ë¡œê·¸ ë§ìŒ
                entry_difficulty = EntryDifficulty.HARD
            else:
                # ì¼ë°˜ í‚¤ì›Œë“œ
                if avg_score < 45:
                    entry_difficulty = EntryDifficulty.EASY
                elif avg_score < 60:
                    entry_difficulty = EntryDifficulty.ACHIEVABLE
                elif avg_score < 75:
                    entry_difficulty = EntryDifficulty.HARD
                else:
                    entry_difficulty = EntryDifficulty.VERY_HARD

            # ê¶Œì¥ ë¸”ë¡œê·¸ ì ìˆ˜ (ìƒìœ„ ì§„ì…ì„ ìœ„í•´)
            # ì§€ì—­+ì˜ë£Œ í‚¤ì›Œë“œëŠ” ë” ë†’ì€ ì ìˆ˜ í•„ìš”
            if is_local_medical:
                recommended_score = top10_stats.avg_total_score * 1.1  # í‰ê· ë³´ë‹¤ 10% ì´ìƒ
            else:
                recommended_score = top10_stats.min_score * 0.9 if top10_stats.min_score else 50

            # ë‚œì´ë„ ì‚¬ìœ  ìƒì„±
            difficulty_reason = None
            if is_local_medical and official_blog_ratio >= 0.5:
                difficulty_reason = f"ì§€ì—­+ì˜ë£Œ í‚¤ì›Œë“œì´ë©°, ìƒìœ„ 10ìœ„ ì¤‘ {official_blog_ratio:.0%}ê°€ ë³‘ì› ê³µì‹ ë¸”ë¡œê·¸ì…ë‹ˆë‹¤."
            elif is_local_medical:
                difficulty_reason = "ì§€ì—­+ì˜ë£Œ ë³µí•© í‚¤ì›Œë“œë¡œ, ë³‘ì› ê³µì‹ ë¸”ë¡œê·¸ê°€ ìƒìœ„ë¥¼ ì°¨ì§€í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤."
            elif official_blog_ratio >= 0.5:
                difficulty_reason = f"ìƒìœ„ 10ìœ„ ì¤‘ {official_blog_ratio:.0%}ê°€ ë³‘ì›/ì˜ì› ê³µì‹ ë¸”ë¡œê·¸ì…ë‹ˆë‹¤."
            elif avg_score >= 75:
                difficulty_reason = f"ìƒìœ„ ë¸”ë¡œê·¸ë“¤ì˜ í‰ê·  ì ìˆ˜ê°€ {avg_score:.0f}ì ìœ¼ë¡œ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤."

            analysis = CompetitionAnalysis(
                keyword=keyword,
                search_volume=search_result.insights.get('total_search_volume', 0) if hasattr(search_result, 'insights') else 0,
                competition_level=competition_level,
                top10_stats=top10_stats,
                tab_ratio=tab_ratio,
                entry_difficulty=entry_difficulty,
                recommended_blog_score=recommended_score,
                is_medical_keyword=is_medical,
                is_local_medical=is_local_medical,
                official_blog_ratio=official_blog_ratio,
                difficulty_reason=difficulty_reason
            )

            # ì¶”ê°€ ë©”íƒ€ë°ì´í„° (ë¡œê¹…ìš©)
            logger.info(f"Competition analysis for '{keyword}': "
                       f"is_local_medical={is_local_medical}, "
                       f"official_blog_ratio={official_blog_ratio:.1%}, "
                       f"entry_difficulty={entry_difficulty.value}")

            # ì´ë ¥ ì €ì¥
            save_competition_history(keyword, analysis.model_dump())

            return analysis

        except Exception as e:
            logger.error(f"Error analyzing competition for '{keyword}': {e}")
            return CompetitionAnalysis(keyword=keyword)

    async def get_tab_ratio(self, keyword: str) -> TabRatio:
        """ë„¤ì´ë²„ íƒ­ë³„ ê²€ìƒ‰ ë¹„ìœ¨ ì¡°íšŒ"""
        # ìºì‹œ í™•ì¸
        cached = get_cached_tab_ratio(keyword)
        if cached:
            total = cached['total_count'] or 1
            return TabRatio(
                blog=cached['blog_count'] / total,
                cafe=cached['cafe_count'] / total,
                kin=cached['kin_count'] / total,
                web=cached['web_count'] / total,
                blog_count=cached['blog_count'],
                cafe_count=cached['cafe_count'],
                kin_count=cached['kin_count'],
                web_count=cached['web_count']
            )

        # ë„¤ì´ë²„ Open APIë¡œ ê° íƒ­ ê²€ìƒ‰
        try:
            counts = await self._fetch_tab_counts(keyword)

            total = sum(counts.values()) or 1
            tab_ratio = TabRatio(
                blog=counts['blog'] / total,
                cafe=counts['cafe'] / total,
                kin=counts['kin'] / total,
                web=counts['web'] / total,
                blog_count=counts['blog'],
                cafe_count=counts['cafe'],
                kin_count=counts['kin'],
                web_count=counts['web']
            )

            # ìºì‹œ ì €ì¥
            cache_tab_ratio(keyword, counts['blog'], counts['cafe'], counts['kin'], counts['web'])

            return tab_ratio

        except Exception as e:
            logger.error(f"Error fetching tab ratio: {e}")
            return TabRatio()

    async def _fetch_tab_counts(self, keyword: str) -> Dict[str, int]:
        """ë„¤ì´ë²„ Open APIë¡œ íƒ­ë³„ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì¡°íšŒ"""
        client = await self._get_client()

        endpoints = {
            'blog': 'https://openapi.naver.com/v1/search/blog.json',
            'cafe': 'https://openapi.naver.com/v1/search/cafearticle.json',
            'kin': 'https://openapi.naver.com/v1/search/kin.json',
            'web': 'https://openapi.naver.com/v1/search/webkr.json'
        }

        headers = {
            "X-Naver-Client-Id": settings.NAVER_CLIENT_ID,
            "X-Naver-Client-Secret": settings.NAVER_CLIENT_SECRET
        }

        counts = {}

        async def fetch_one(tab: str, url: str):
            try:
                response = await client.get(
                    url,
                    headers=headers,
                    params={"query": keyword, "display": 1}
                )
                if response.status_code == 200:
                    data = response.json()
                    return (tab, data.get("total", 0))
                return (tab, 0)
            except Exception as e:
                logger.error(f"Error fetching {tab} count: {e}")
                return (tab, 0)

        # ë³‘ë ¬ ì¡°íšŒ
        tasks = [fetch_one(tab, url) for tab, url in endpoints.items()]
        results = await asyncio.gather(*tasks)

        for tab, count in results:
            counts[tab] = count

        return counts

    def _calculate_type_distribution(self, keywords: List[KeywordData]) -> Dict[str, int]:
        """ìœ í˜•ë³„ ë¶„í¬ ê³„ì‚°"""
        distribution = defaultdict(int)
        for kw in keywords:
            distribution[kw.keyword_type.value] += 1
        return dict(distribution)

    def _generate_recommendations(
        self,
        competition: CompetitionAnalysis,
        type_dist: Dict[str, int]
    ) -> List[str]:
        """ì¶”ì²œ ë©”ì‹œì§€ ìƒì„± (ì˜ë£Œ/ì§€ì—­ í‚¤ì›Œë“œ ê²½ê³  ê°•í™”)"""
        recommendations = []
        keyword = competition.keyword

        # ì˜ë£Œ/ì§€ì—­ í‚¤ì›Œë“œ íŠ¹ìˆ˜ ê²½ê³ 
        is_local_medical = self.classifier.is_local_medical_keyword(keyword)
        is_medical, medical_matches = self.classifier.is_medical_keyword(keyword)

        if is_local_medical:
            recommendations.append("âš ï¸ ì§€ì—­+ì˜ë£Œ í‚¤ì›Œë“œì…ë‹ˆë‹¤. ë³‘ì›/ì˜ì› ê³µì‹ ë¸”ë¡œê·¸ê°€ ìƒìœ„ë¥¼ ì°¨ì§€í•˜ëŠ” ê²½ìš°ê°€ ë§ì•„ ì¼ë°˜ ë¸”ë¡œê±°ì˜ ìƒìœ„ ë…¸ì¶œì´ ë§¤ìš° ì–´ë µìŠµë‹ˆë‹¤.")
            recommendations.append(f"ğŸ’¡ ëŒ€ì•ˆ: í›„ê¸°/ê²½í—˜ ì¤‘ì‹¬ì˜ ë¡±í…Œì¼ í‚¤ì›Œë“œ(ì˜ˆ: '{keyword} í›„ê¸°', '{keyword} ê°€ê²©')ë¥¼ ë…¸ë ¤ë³´ì„¸ìš”.")
        elif is_medical:
            recommendations.append("âš ï¸ ì˜ë£Œ ê´€ë ¨ í‚¤ì›Œë“œì…ë‹ˆë‹¤. ë„¤ì´ë²„ëŠ” ì „ë¬¸ì„±(E-E-A-T)ì„ ì¤‘ì‹œí•˜ë¯€ë¡œ ê³µì‹ ì˜ë£Œê¸°ê´€ ë¸”ë¡œê·¸ê°€ ìœ ë¦¬í•©ë‹ˆë‹¤.")

        # ê²½ìŸë„ ê¸°ë°˜ ì¶”ì²œ
        if competition.entry_difficulty == EntryDifficulty.EASY:
            recommendations.append("âœ… ê²½ìŸì´ ë‚®ì€ í‚¤ì›Œë“œì…ë‹ˆë‹¤. ì´ˆë³´ ë¸”ë¡œê±°ë„ ìƒìœ„ ë…¸ì¶œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        elif competition.entry_difficulty == EntryDifficulty.ACHIEVABLE:
            recommendations.append(f"âœ… ë„ì „ ê°€ëŠ¥í•œ í‚¤ì›Œë“œì…ë‹ˆë‹¤. ë¸”ë¡œê·¸ ì ìˆ˜ {competition.recommended_blog_score:.0f}ì  ì´ìƒì´ë©´ ìƒìœ„ ì§„ì… ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
        elif competition.entry_difficulty == EntryDifficulty.HARD:
            recommendations.append(f"âš ï¸ ê²½ìŸì´ ë†’ì€ í‚¤ì›Œë“œì…ë‹ˆë‹¤. C-Rank ì ìˆ˜ {competition.top10_stats.avg_c_rank:.0f}ì  ì´ìƒì´ í•„ìš”í•©ë‹ˆë‹¤.")
            if not is_local_medical:
                recommendations.append("ğŸ’¡ ì½˜í…ì¸  í’ˆì§ˆ(ì´ë¯¸ì§€ 15ê°œ+, ë³¸ë¬¸ 3000ì+, ì†Œì œëª© 10ê°œ+)ì— ì§‘ì¤‘í•˜ì„¸ìš”.")
        else:
            recommendations.append("ğŸš« ë§¤ìš° ê²½ìŸì´ ë†’ì€ í‚¤ì›Œë“œì…ë‹ˆë‹¤. ìƒìœ„ ë¸”ë¡œê±°ë“¤ì˜ í‰ê·  ì ìˆ˜ê°€ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤.")
            if is_local_medical:
                recommendations.append("ğŸ’¡ ì´ í‚¤ì›Œë“œëŠ” ê³µì‹ ë³‘ì› ë¸”ë¡œê·¸ ìœ„ì£¼ë¡œ ë…¸ì¶œë©ë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.")
            else:
                recommendations.append("ğŸ’¡ ë¡±í…Œì¼ í‚¤ì›Œë“œë‚˜ í‹ˆìƒˆ í‚¤ì›Œë“œë¥¼ ì°¾ì•„ë³´ì„¸ìš”.")

        # ìœ í˜•ë³„ ì¶”ì²œ
        total = sum(type_dist.values())
        if total > 0:
            info_ratio = type_dist.get("ì •ë³´í˜•", 0) / total
            if info_ratio > 0.4:
                recommendations.append("ğŸ“ ì •ë³´í˜• í‚¤ì›Œë“œê°€ ë§ìŠµë‹ˆë‹¤. ìƒì„¸í•œ ì •ë³´ ì œê³µ ì½˜í…ì¸ ê°€ íš¨ê³¼ì ì…ë‹ˆë‹¤.")

            local_ratio = (type_dist.get("ì§€ì—­í˜•", 0) + type_dist.get("ê´‘ì—­í˜•", 0)) / total
            if local_ratio > 0.3:
                recommendations.append("ğŸ“ ì§€ì—­ í‚¤ì›Œë“œê°€ ë§ìŠµë‹ˆë‹¤. ì§€ì—­ëª…+í•µì‹¬í‚¤ì›Œë“œ ì¡°í•©ìœ¼ë¡œ ìµœì í™”í•˜ì„¸ìš”.")

        # íƒ­ ë¹„ìœ¨ ì¶”ì²œ
        if competition.tab_ratio.blog > 0.4:
            recommendations.append("ğŸ“Š ë¸”ë¡œê·¸ ì½˜í…ì¸ ê°€ ë§ì´ ë…¸ì¶œë˜ëŠ” í‚¤ì›Œë“œì…ë‹ˆë‹¤. ë¸”ë¡œê·¸ SEOì— ì§‘ì¤‘í•˜ì„¸ìš”.")
        elif competition.tab_ratio.cafe > 0.3:
            recommendations.append("ğŸ“Š ì¹´í˜ ì½˜í…ì¸ ê°€ ë§ìŠµë‹ˆë‹¤. ë„¤ì´ë²„ ì¹´í˜ í™œë™ë„ ê³ ë ¤í•´ë³´ì„¸ìš”.")

        return recommendations


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
keyword_analysis_service = KeywordAnalysisService()
