"""
Automatic backup service for learning data persistence
ë°ì´í„° ì˜êµ¬ ë³´ì¡´ì„ ìœ„í•œ ìë™ ë°±ì—… ì„œë¹„ìŠ¤
"""
import os
import json
import shutil
import sqlite3
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import threading
import time
import logging

logger = logging.getLogger(__name__)

# Backup configuration
# Windows ë¡œì»¬ ê°œë°œí™˜ê²½ì—ì„œëŠ” ./data ì‚¬ìš©
import sys
if sys.platform == "win32":
    BACKUP_DIR = os.path.join(os.path.dirname(__file__), "..", "data", "backups")
    DATABASE_PATH = os.path.join(os.path.dirname(__file__), "..", "data", "blog_analyzer.db")
else:
    BACKUP_DIR = "/data/backups"
    DATABASE_PATH = "/data/blog_analyzer.db"
MAX_BACKUPS = 8  # 16ì‹œê°„ ë¶„ëŸ‰ (2ì‹œê°„ë§ˆë‹¤ ë°±ì—…) - ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
MAX_JSON_BACKUPS = 2  # JSON ë°±ì—…ì€ 2ê°œë§Œ ìœ ì§€ - ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
BACKUP_INTERVAL_SECONDS = 7200  # 2ì‹œê°„ë§ˆë‹¤ (ë¦¬ì†ŒìŠ¤ ì ˆì•½)
DISK_WARNING_THRESHOLD_MB = 100  # 100MB ì´í•˜ë©´ ê²½ê³ 


def ensure_backup_dir():
    """ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±"""
    os.makedirs(BACKUP_DIR, exist_ok=True)


def create_backup() -> Optional[str]:
    """
    ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ìƒì„±
    Returns: ë°±ì—… íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None
    """
    try:
        ensure_backup_dir()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = os.path.join(BACKUP_DIR, f"backup_{timestamp}.db")

        # SQLite ì•ˆì „í•œ ë°±ì—… (VACUUM INTO ì‚¬ìš©)
        if os.path.exists(DATABASE_PATH):
            conn = sqlite3.connect(DATABASE_PATH)
            backup_conn = sqlite3.connect(backup_path)
            conn.backup(backup_conn)
            backup_conn.close()
            conn.close()

            logger.info(f"Backup created: {backup_path}")
            return backup_path
        else:
            logger.warning(f"Database not found: {DATABASE_PATH}")
            return None

    except Exception as e:
        logger.error(f"Backup failed: {e}")
        return None


def get_disk_free_space_mb() -> float:
    """ë°±ì—… ë””ë ‰í† ë¦¬ì˜ ë‚¨ì€ ë””ìŠ¤í¬ ê³µê°„ (MB)"""
    try:
        if sys.platform == "win32":
            import ctypes
            free_bytes = ctypes.c_ulonglong(0)
            ctypes.windll.kernel32.GetDiskFreeSpaceExW(
                ctypes.c_wchar_p(BACKUP_DIR), None, None, ctypes.pointer(free_bytes)
            )
            return free_bytes.value / (1024 * 1024)
        else:
            stat = os.statvfs(BACKUP_DIR)
            return (stat.f_bavail * stat.f_frsize) / (1024 * 1024)
    except Exception as e:
        logger.error(f"Failed to get disk space: {e}")
        return -1


def cleanup_old_backups():
    """ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ì •ë¦¬ (MAX_BACKUPS ìœ ì§€)"""
    try:
        ensure_backup_dir()

        # 1. DB ë°±ì—… íŒŒì¼ ì •ë¦¬
        db_backups = sorted([
            f for f in os.listdir(BACKUP_DIR)
            if f.startswith("backup_") and f.endswith(".db")
        ])

        while len(db_backups) > MAX_BACKUPS:
            old_backup = db_backups.pop(0)
            old_path = os.path.join(BACKUP_DIR, old_backup)
            try:
                os.remove(old_path)
                logger.info(f"Removed old backup: {old_backup}")
            except Exception as e:
                logger.warning(f"Failed to remove {old_backup}: {e}")

        # 2. DB Journal íŒŒì¼ ì •ë¦¬ (backupê³¼ ìŒì´ ì—†ëŠ” ê²ƒë“¤ ì‚­ì œ)
        for f in os.listdir(BACKUP_DIR):
            if f.endswith(".db-journal"):
                db_name = f.replace("-journal", "")
                if db_name not in db_backups:
                    try:
                        os.remove(os.path.join(BACKUP_DIR, f))
                        logger.info(f"Removed orphan journal: {f}")
                    except Exception as e:
                        logger.warning(f"Failed to remove journal {f}: {e}")

        # 3. JSON ë°±ì—… íŒŒì¼ ì •ë¦¬ (MAX_JSON_BACKUPS ìœ ì§€)
        cleanup_old_json_backups()

        # 4. ë””ìŠ¤í¬ ê³µê°„ ì²´í¬
        free_space = get_disk_free_space_mb()
        if 0 < free_space < DISK_WARNING_THRESHOLD_MB:
            logger.warning(f"âš ï¸ Low disk space: {free_space:.1f}MB remaining")
            # ê¸´ê¸‰ ì •ë¦¬: ì¶”ê°€ë¡œ ë°±ì—… ì‚­ì œ
            emergency_cleanup()

    except Exception as e:
        logger.error(f"Cleanup failed: {e}")


def cleanup_old_json_backups():
    """ì˜¤ë˜ëœ JSON ë°±ì—… íŒŒì¼ ì •ë¦¬ (MAX_JSON_BACKUPS ìœ ì§€)"""
    try:
        ensure_backup_dir()

        json_backups = sorted([
            f for f in os.listdir(BACKUP_DIR)
            if f.startswith("learning_data_") and f.endswith(".json")
        ])

        while len(json_backups) > MAX_JSON_BACKUPS:
            old_json = json_backups.pop(0)
            old_path = os.path.join(BACKUP_DIR, old_json)
            try:
                os.remove(old_path)
                logger.info(f"Removed old JSON backup: {old_json}")
            except Exception as e:
                logger.warning(f"Failed to remove JSON {old_json}: {e}")

    except Exception as e:
        logger.error(f"JSON cleanup failed: {e}")


def emergency_cleanup():
    """ê¸´ê¸‰ ë””ìŠ¤í¬ ê³µê°„ í™•ë³´ - ë°±ì—… ìˆ˜ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì„"""
    try:
        logger.warning("ğŸš¨ Emergency cleanup triggered due to low disk space")

        # DB ë°±ì—…ì„ ì ˆë°˜ìœ¼ë¡œ
        db_backups = sorted([
            f for f in os.listdir(BACKUP_DIR)
            if f.startswith("backup_") and f.endswith(".db")
        ])

        target_count = max(6, len(db_backups) // 2)  # ìµœì†Œ 6ê°œëŠ” ìœ ì§€
        while len(db_backups) > target_count:
            old_backup = db_backups.pop(0)
            try:
                os.remove(os.path.join(BACKUP_DIR, old_backup))
                # ê´€ë ¨ journalë„ ì‚­ì œ
                journal_path = os.path.join(BACKUP_DIR, old_backup + "-journal")
                if os.path.exists(journal_path):
                    os.remove(journal_path)
                logger.info(f"Emergency removed: {old_backup}")
            except Exception:
                pass

        # JSON ë°±ì—…ì„ 2ê°œë¡œ
        json_backups = sorted([
            f for f in os.listdir(BACKUP_DIR)
            if f.startswith("learning_data_") and f.endswith(".json")
        ])

        while len(json_backups) > 2:
            old_json = json_backups.pop(0)
            try:
                os.remove(os.path.join(BACKUP_DIR, old_json))
                logger.info(f"Emergency removed JSON: {old_json}")
            except Exception:
                pass

        free_space = get_disk_free_space_mb()
        logger.info(f"After emergency cleanup: {free_space:.1f}MB free")

    except Exception as e:
        logger.error(f"Emergency cleanup failed: {e}")


def export_to_json() -> Optional[str]:
    """
    í•™ìŠµ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸° (ì™¸ë¶€ ë°±ì—…ìš©)
    Returns: JSON íŒŒì¼ ê²½ë¡œ
    """
    try:
        ensure_backup_dir()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_path = os.path.join(BACKUP_DIR, f"learning_data_{timestamp}.json")

        if not os.path.exists(DATABASE_PATH):
            return None

        conn = sqlite3.connect(DATABASE_PATH)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()

        export_data = {
            "exported_at": datetime.now().isoformat(),
            "version": "1.0",
            "data": {}
        }

        # í•™ìŠµ ìƒ˜í”Œ ë‚´ë³´ë‚´ê¸°
        cursor.execute("SELECT * FROM learning_samples ORDER BY collected_at DESC")
        export_data["data"]["learning_samples"] = [dict(row) for row in cursor.fetchall()]

        # í•™ìŠµ ì„¸ì…˜ ë‚´ë³´ë‚´ê¸°
        cursor.execute("SELECT * FROM learning_sessions ORDER BY started_at DESC")
        export_data["data"]["learning_sessions"] = [dict(row) for row in cursor.fetchall()]

        # ê°€ì¤‘ì¹˜ íˆìŠ¤í† ë¦¬ ë‚´ë³´ë‚´ê¸°
        cursor.execute("SELECT * FROM weight_history ORDER BY created_at DESC")
        export_data["data"]["weight_history"] = [dict(row) for row in cursor.fetchall()]

        # í˜„ì¬ ê°€ì¤‘ì¹˜ ë‚´ë³´ë‚´ê¸°
        cursor.execute("SELECT * FROM current_weights")
        export_data["data"]["current_weights"] = [dict(row) for row in cursor.fetchall()]

        conn.close()

        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2, default=str)

        logger.info(f"JSON export created: {json_path}")

        # ì˜¤ë˜ëœ JSON ë°±ì—… ì •ë¦¬
        cleanup_old_json_backups()

        return json_path

    except Exception as e:
        logger.error(f"JSON export failed: {e}")
        return None


def import_from_json(json_path: str) -> bool:
    """
    JSONì—ì„œ í•™ìŠµ ë°ì´í„° ë³µì›
    """
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            import_data = json.load(f)

        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()

        # í•™ìŠµ ìƒ˜í”Œ ë³µì›
        samples = import_data.get("data", {}).get("learning_samples", [])
        for sample in samples:
            cursor.execute("""
                INSERT OR IGNORE INTO learning_samples (
                    id, keyword, blog_id, actual_rank, predicted_score,
                    c_rank_score, dia_score, post_count, neighbor_count,
                    blog_age_days, recent_posts_30d, visitor_count, collected_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                sample.get('id'),
                sample.get('keyword'),
                sample.get('blog_id'),
                sample.get('actual_rank'),
                sample.get('predicted_score'),
                sample.get('c_rank_score'),
                sample.get('dia_score'),
                sample.get('post_count'),
                sample.get('neighbor_count'),
                sample.get('blog_age_days'),
                sample.get('recent_posts_30d'),
                sample.get('visitor_count'),
                sample.get('collected_at')
            ))

        # í˜„ì¬ ê°€ì¤‘ì¹˜ ë³µì›
        weights = import_data.get("data", {}).get("current_weights", [])
        for w in weights:
            cursor.execute("""
                INSERT OR REPLACE INTO current_weights (id, weights, updated_at)
                VALUES (?, ?, ?)
            """, (w.get('id', 1), w.get('weights'), w.get('updated_at')))

        conn.commit()
        conn.close()

        logger.info(f"Data imported from: {json_path}")
        return True

    except Exception as e:
        logger.error(f"Import failed: {e}")
        return False


def get_backup_list() -> List[Dict]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—… ëª©ë¡ ì¡°íšŒ"""
    try:
        ensure_backup_dir()
        backups = []

        for f in sorted(os.listdir(BACKUP_DIR), reverse=True):
            if f.startswith("backup_") and f.endswith(".db"):
                path = os.path.join(BACKUP_DIR, f)
                stat = os.stat(path)
                backups.append({
                    "filename": f,
                    "path": path,
                    "size_mb": round(stat.st_size / (1024 * 1024), 2),
                    "created_at": datetime.fromtimestamp(stat.st_mtime).isoformat()
                })

        return backups

    except Exception as e:
        logger.error(f"Failed to list backups: {e}")
        return []


def restore_from_backup(backup_filename: str) -> bool:
    """
    ë°±ì—…ì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ë³µì›
    """
    try:
        backup_path = os.path.join(BACKUP_DIR, backup_filename)

        if not os.path.exists(backup_path):
            logger.error(f"Backup not found: {backup_path}")
            return False

        # í˜„ì¬ DB ë°±ì—…
        if os.path.exists(DATABASE_PATH):
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            shutil.copy(DATABASE_PATH, f"{DATABASE_PATH}.pre_restore_{timestamp}")

        # ë°±ì—…ì—ì„œ ë³µì›
        shutil.copy(backup_path, DATABASE_PATH)
        logger.info(f"Restored from: {backup_filename}")
        return True

    except Exception as e:
        logger.error(f"Restore failed: {e}")
        return False


def get_backup_status() -> Dict:
    """ë°±ì—… ìƒíƒœ ì¡°íšŒ"""
    try:
        backups = get_backup_list()
        latest = backups[0] if backups else None

        return {
            "enabled": True,
            "backup_count": len(backups),
            "latest_backup": latest,
            "backup_dir": BACKUP_DIR,
            "max_backups": MAX_BACKUPS,
            "interval_hours": BACKUP_INTERVAL_SECONDS / 3600
        }

    except Exception as e:
        return {
            "enabled": False,
            "error": str(e)
        }


class BackupScheduler:
    """ìë™ ë°±ì—… ìŠ¤ì¼€ì¤„ëŸ¬"""

    def __init__(self):
        self.running = False
        self.thread = None

    def start(self):
        """ë°±ì—… ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
        if self.running:
            return

        self.running = True
        self.thread = threading.Thread(target=self._run_scheduler, daemon=True)
        self.thread.start()
        logger.info("Backup scheduler started")

    def stop(self):
        """ë°±ì—… ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€ (ì¦‰ì‹œ)"""
        self.running = False
        if self.thread:
            self.thread.join(timeout=2)  # ë¹ ë¥¸ ì¢…ë£Œ
        logger.info("Backup scheduler stopped")

    def _run_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ë©”ì¸ ë£¨í”„"""
        # ì‹œì‘ ì‹œ ì¦‰ì‹œ ë°±ì—… ìƒì„±
        create_backup()
        cleanup_old_backups()

        while self.running:
            time.sleep(BACKUP_INTERVAL_SECONDS)
            if self.running:
                create_backup()
                cleanup_old_backups()
                # ë§¤ 6ì‹œê°„ë§ˆë‹¤ JSON ë‚´ë³´ë‚´ê¸°
                hour = datetime.now().hour
                if hour % 6 == 0:
                    export_to_json()


# ì „ì—­ ìŠ¤ì¼€ì¤„ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
backup_scheduler = BackupScheduler()
