"""
ìë™ í•™ìŠµ ìŠ¤ì¼€ì¤„ëŸ¬ (Auto Learning Scheduler)
- ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì§€ì†ì ìœ¼ë¡œ í‚¤ì›Œë“œ í•™ìŠµ
- ë„¤ì´ë²„ ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ì†ë„ ì¡°ì ˆ
- í•™ìŠµ ë°ì´í„°ê°€ ìŒ“ì´ë©´ ìë™ ëª¨ë¸ ì—…ë°ì´íŠ¸
"""
import asyncio
import threading
import time
import random
import logging
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional
import json

logger = logging.getLogger(__name__)

# ==============================================
# ì„¤ì •
# ==============================================
AUTO_LEARNING_CONFIG = {
    "enabled": True,                    # ìë™ í•™ìŠµ í™œì„±í™”
    "interval_minutes": 10,             # í•™ìŠµ ì£¼ê¸° (ë¶„)
    "keywords_per_cycle": 2,            # í•œ ë²ˆì— í•™ìŠµí•  í‚¤ì›Œë“œ ìˆ˜ (1 â†’ 2)
    "blogs_per_keyword": 10,            # í‚¤ì›Œë“œë‹¹ ë¶„ì„í•  ë¸”ë¡œê·¸ ìˆ˜ (5 â†’ 10ìœ¼ë¡œ ì¦ê°€)
    "delay_between_keywords": 5.0,      # í‚¤ì›Œë“œ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    "delay_between_blogs": 2.0,         # ë¸”ë¡œê·¸ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ) - 3 â†’ 2ë¡œ ê°ì†Œ (í•™ìŠµ ì†ë„ í–¥ìƒ)
    "auto_train_threshold": 50,         # ìë™ í›ˆë ¨ íŠ¸ë¦¬ê±° ìƒ˜í”Œ ìˆ˜
    "quiet_hours_start": 2,             # ì¡°ìš©í•œ ì‹œê°„ ì‹œì‘ (ì„œë²„ ë¶€í•˜ ê°ì†Œ)
    "quiet_hours_end": 6,               # ì¡°ìš©í•œ ì‹œê°„ ë
    "quiet_hours_interval": 30,         # ì¡°ìš©í•œ ì‹œê°„ëŒ€ í•™ìŠµ ì£¼ê¸° (ë¶„) - 60 â†’ 30ìœ¼ë¡œ ê°ì†Œ
    "daily_training_hour": 3,           # ë§¤ì¼ ëŒ€ê·œëª¨ í›ˆë ¨ ì‹œê°„ (UTC, í•œêµ­ì‹œê°„ 12ì‹œ)
    "daily_training_samples": 5000,     # ëŒ€ê·œëª¨ í›ˆë ¨ ì‹œ ì‚¬ìš©í•  ìƒ˜í”Œ ìˆ˜
}

# í•™ìŠµ ìƒíƒœ
auto_learning_state = {
    "is_running": False,
    "is_enabled": True,
    "last_run": None,
    "next_run": None,
    "total_keywords_learned": 0,
    "total_blogs_analyzed": 0,
    "total_cycles": 0,
    "errors": [],
    "current_keyword": None,
    "samples_since_last_train": 0,
    "last_daily_training": None,
    "daily_training_accuracy": None,
}

# ==============================================
# DB ê¸°ë°˜ í‚¤ì›Œë“œ ê´€ë¦¬ (ì„œë²„ ì¬ì‹œì‘í•´ë„ ìœ ì§€)
# ==============================================


def get_next_keywords(count: int) -> List[str]:
    """ë‹¤ìŒ í•™ìŠµí•  í‚¤ì›Œë“œ ì„ íƒ (DB ê¸°ë°˜, ì¤‘ë³µ ë°©ì§€ ê°•í™”)"""
    try:
        from database.learning_db import (
            get_next_keywords_from_pool,
            get_keyword_learning_stats,
            initialize_default_keyword_pool,
            get_unlearned_keywords,
            add_to_keyword_pool
        )

        # í‚¤ì›Œë“œ í’€ ìƒíƒœ í™•ì¸
        stats = get_keyword_learning_stats()

        # í‚¤ì›Œë“œ í’€ì´ ë¹„ì–´ìˆê±°ë‚˜ ë¯¸í•™ìŠµ í‚¤ì›Œë“œê°€ ë¶€ì¡±í•˜ë©´ ì´ˆê¸°í™”/í™•ì¥
        if stats.get("total_pool", 0) == 0:
            logger.info("[AutoLearn] Initializing keyword pool...")
            added = initialize_default_keyword_pool()
            logger.info(f"[AutoLearn] Added {added} keywords to pool")
        elif stats.get("never_learned", 0) < 50:
            # ë¯¸í•™ìŠµ í‚¤ì›Œë“œê°€ 50ê°œ ë¯¸ë§Œì´ë©´ ìƒˆ í‚¤ì›Œë“œ ì¶”ê°€
            logger.info("[AutoLearn] Adding more keywords to pool...")
            _add_trending_keywords()

        # 1ìˆœìœ„: í•œ ë²ˆë„ í•™ìŠµ ì•ˆëœ í‚¤ì›Œë“œë§Œ ê°€ì ¸ì˜¤ê¸°
        keywords = get_unlearned_keywords(limit=count)

        if keywords:
            logger.info(f"[AutoLearn] Found {len(keywords)} unlearned keywords")
            return keywords

        # 2ìˆœìœ„: 30ì¼ ì´ìƒ ê²½ê³¼í•œ í‚¤ì›Œë“œ (ì¬í•™ìŠµ)
        keywords = get_next_keywords_from_pool(count, min_days_since_last=30)

        if keywords:
            logger.info(f"[AutoLearn] Relearning old keywords (30+ days): {keywords}")
            return keywords

        # 3ìˆœìœ„: ìƒˆ í‚¤ì›Œë“œ ìë™ ìƒì„± í›„ í•™ìŠµ
        logger.info("[AutoLearn] All keywords learned, generating new keywords...")
        new_keywords = _generate_new_keywords(count)
        for kw in new_keywords:
            add_to_keyword_pool(kw, category="auto_generated", source="auto", priority=5)
        return new_keywords

    except Exception as e:
        logger.error(f"[AutoLearn] Error getting next keywords: {e}")
        import traceback
        traceback.print_exc()
        # í´ë°±: ëœë¤ í‚¤ì›Œë“œ ìƒì„±
        return _generate_new_keywords(count)


def _add_trending_keywords():
    """íŠ¸ë Œë”©/ì¸ê¸° í‚¤ì›Œë“œ ì¶”ê°€"""
    try:
        from database.learning_db import add_to_keyword_pool

        # ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ì˜ í‚¤ì›Œë“œ ì¶”ê°€
        trending_keywords = [
            # ë·°í‹°/í™”ì¥í’ˆ
            ("ì„ í¬ë¦¼ì¶”ì²œ", "ë·°í‹°"), ("íŒŒìš´ë°ì´ì…˜ì¶”ì²œ", "ë·°í‹°"), ("ë¦½ìŠ¤í‹±ì¶”ì²œ", "ë·°í‹°"),
            ("ìŠ¤í‚¨ì¼€ì–´", "ë·°í‹°"), ("í´ë Œì§•í¼", "ë·°í‹°"), ("í† ë„ˆì¶”ì²œ", "ë·°í‹°"),
            ("ì„¸ëŸ¼ì¶”ì²œ", "ë·°í‹°"), ("ë§ˆìŠ¤í¬íŒ©", "ë·°í‹°"), ("ì•„ì´ë¼ì´ë„ˆ", "ë·°í‹°"),
            # ë§›ì§‘
            ("ê°•ë‚¨ë§›ì§‘", "ë§›ì§‘"), ("í™ëŒ€ë§›ì§‘", "ë§›ì§‘"), ("ì´íƒœì›ë§›ì§‘", "ë§›ì§‘"),
            ("ì„±ìˆ˜ë§›ì§‘", "ë§›ì§‘"), ("ì—¬ì˜ë„ë§›ì§‘", "ë§›ì§‘"), ("íŒêµë§›ì§‘", "ë§›ì§‘"),
            ("ë¶„ë‹¹ë§›ì§‘", "ë§›ì§‘"), ("ì¼ì‚°ë§›ì§‘", "ë§›ì§‘"), ("ìˆ˜ì›ë§›ì§‘", "ë§›ì§‘"),
            # ì—¬í–‰
            ("ì œì£¼ì—¬í–‰", "ì—¬í–‰"), ("ë¶€ì‚°ì—¬í–‰", "ì—¬í–‰"), ("ê°•ë¦‰ì—¬í–‰", "ì—¬í–‰"),
            ("ê²½ì£¼ì—¬í–‰", "ì—¬í–‰"), ("ì „ì£¼ì—¬í–‰", "ì—¬í–‰"), ("ì†ì´ˆì—¬í–‰", "ì—¬í–‰"),
            ("ì¼ë³¸ì—¬í–‰", "ì—¬í–‰"), ("ë² íŠ¸ë‚¨ì—¬í–‰", "ì—¬í–‰"), ("íƒœêµ­ì—¬í–‰", "ì—¬í–‰"),
            # ê±´ê°•/ì˜ë£Œ
            ("ë‹¤ì´ì–´íŠ¸", "ê±´ê°•"), ("í—¬ìŠ¤ì¥ì¶”ì²œ", "ê±´ê°•"), ("í•„ë¼í…ŒìŠ¤", "ê±´ê°•"),
            ("ìš”ê°€", "ê±´ê°•"), ("ì˜ì–‘ì œì¶”ì²œ", "ê±´ê°•"), ("ë¹„íƒ€ë¯¼ì¶”ì²œ", "ê±´ê°•"),
            # IT/ì „ì
            ("ë…¸íŠ¸ë¶ì¶”ì²œ", "IT"), ("ìŠ¤ë§ˆíŠ¸í°ì¶”ì²œ", "IT"), ("íƒœë¸”ë¦¿ì¶”ì²œ", "IT"),
            ("ì´ì–´í°ì¶”ì²œ", "IT"), ("ëª¨ë‹ˆí„°ì¶”ì²œ", "IT"), ("í‚¤ë³´ë“œì¶”ì²œ", "IT"),
            # ìœ¡ì•„/êµìœ¡
            ("ì˜ì–´í•™ì›", "êµìœ¡"), ("ìˆ˜í•™í•™ì›", "êµìœ¡"), ("ìœ ì•„êµìœ¡", "êµìœ¡"),
            ("ì–´ë¦°ì´ì§‘", "êµìœ¡"), ("ì´ˆë“±í•™êµ", "êµìœ¡"), ("ì¤‘í•™êµ", "êµìœ¡"),
            # ë°˜ë ¤ë™ë¬¼
            ("ê°•ì•„ì§€ì‚¬ë£Œ", "ë°˜ë ¤ë™ë¬¼"), ("ê³ ì–‘ì´ì‚¬ë£Œ", "ë°˜ë ¤ë™ë¬¼"), ("ë™ë¬¼ë³‘ì›", "ë°˜ë ¤ë™ë¬¼"),
            # ì¸í…Œë¦¬ì–´
            ("ì¸í…Œë¦¬ì–´", "ì¸í…Œë¦¬ì–´"), ("ê°€êµ¬ì¶”ì²œ", "ì¸í…Œë¦¬ì–´"), ("ì¡°ëª…ì¶”ì²œ", "ì¸í…Œë¦¬ì–´"),
            # ìë™ì°¨
            ("ìë™ì°¨ì¶”ì²œ", "ìë™ì°¨"), ("ì „ê¸°ì°¨", "ìë™ì°¨"), ("íƒ€ì´ì–´ì¶”ì²œ", "ìë™ì°¨"),
        ]

        added = 0
        for keyword, category in trending_keywords:
            if add_to_keyword_pool(keyword, category=category, source="trending", priority=3):
                added += 1

        logger.info(f"[AutoLearn] Added {added} trending keywords")

    except Exception as e:
        logger.error(f"[AutoLearn] Error adding trending keywords: {e}")


def _generate_new_keywords(count: int) -> List[str]:
    """ìƒˆ í‚¤ì›Œë“œ ìë™ ìƒì„± (ì¡°í•© ë°©ì‹)"""
    import random

    prefixes = [
        "ê°•ë‚¨", "í™ëŒ€", "ì‹ ì´Œ", "ëª…ë™", "ì´íƒœì›", "ì„±ìˆ˜", "íŒêµ", "ë¶„ë‹¹",
        "ìˆ˜ì›", "ì¸ì²œ", "ëŒ€ì „", "ëŒ€êµ¬", "ë¶€ì‚°", "ê´‘ì£¼", "ì œì£¼"
    ]

    suffixes = [
        "ë§›ì§‘", "ì¹´í˜", "í”¼ë¶€ê³¼", "ì¹˜ê³¼", "í—¬ìŠ¤ì¥", "í•„ë¼í…ŒìŠ¤", "ìš”ê°€",
        "ë¯¸ìš©ì‹¤", "ë„¤ì¼ìƒµ", "ë§ˆì‚¬ì§€", "ì •í˜•ì™¸ê³¼", "ì•ˆê³¼", "í•œì˜ì›",
        "ë³‘ì›", "í•™ì›", "í˜¸í…”", "ìˆ™ì†Œ", "ê´€ê´‘", "ë°ì´íŠ¸", "ë¸ŒëŸ°ì¹˜"
    ]

    keywords = []
    used = set()

    while len(keywords) < count:
        prefix = random.choice(prefixes)
        suffix = random.choice(suffixes)
        kw = f"{prefix}{suffix}"

        if kw not in used:
            used.add(kw)
            keywords.append(kw)

    return keywords


def mark_keyword_as_learned(keyword: str, samples_count: int = 13):
    """í‚¤ì›Œë“œ í•™ìŠµ ì™„ë£Œ ê¸°ë¡ (DBì— ì €ì¥)"""
    try:
        from database.learning_db import mark_keyword_learned, add_to_keyword_pool

        # í‚¤ì›Œë“œ í’€ì— ì—†ìœ¼ë©´ ì¶”ê°€
        add_to_keyword_pool(keyword, category="auto_added", source="auto")

        # í•™ìŠµ ì™„ë£Œ ê¸°ë¡
        mark_keyword_learned(keyword, samples_count, source="auto")
        logger.debug(f"[AutoLearn] Marked keyword as learned: {keyword}")

    except Exception as e:
        logger.error(f"[AutoLearn] Error marking keyword as learned: {e}")


def get_current_interval() -> int:
    """í˜„ì¬ ì‹œê°„ì— ë§ëŠ” í•™ìŠµ ê°„ê²© ë°˜í™˜ (ë¶„)"""
    hour = datetime.now().hour
    config = AUTO_LEARNING_CONFIG

    # ì¡°ìš©í•œ ì‹œê°„ëŒ€ì—ëŠ” ê°„ê²© ëŠ˜ë¦¼
    if config["quiet_hours_start"] <= hour < config["quiet_hours_end"]:
        return config["quiet_hours_interval"]

    return config["interval_minutes"]


async def run_single_learning_cycle():
    """ë‹¨ì¼ í•™ìŠµ ì‚¬ì´í´ ì‹¤í–‰"""
    global auto_learning_state

    if not auto_learning_state["is_enabled"]:
        return

    auto_learning_state["is_running"] = True
    auto_learning_state["last_run"] = datetime.now(timezone.utc).isoformat()

    try:
        # í•„ìš”í•œ ëª¨ë“ˆ ë™ì  ì„í¬íŠ¸
        from routers.blogs import fetch_naver_search_results, analyze_blog, analyze_post
        from database.learning_db import add_learning_sample, get_learning_samples, get_current_weights, save_current_weights
        from services.learning_engine import instant_adjust_weights

        config = AUTO_LEARNING_CONFIG
        keywords = get_next_keywords(config["keywords_per_cycle"])

        if not keywords:
            logger.info("No new keywords to learn this cycle")
            return

        logger.info(f"[AutoLearn] Starting cycle with keywords: {keywords}")

        for keyword in keywords:
            if not auto_learning_state["is_enabled"]:
                break

            auto_learning_state["current_keyword"] = keyword

            try:
                # ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                search_results = await fetch_naver_search_results(
                    keyword,
                    limit=config["blogs_per_keyword"]
                )

                if not search_results:
                    logger.warning(f"[AutoLearn] No results for: {keyword}")
                    continue

                blogs_analyzed = 0

                for result in search_results:
                    if not auto_learning_state["is_enabled"]:
                        break

                    try:
                        blog_id = result["blog_id"]
                        actual_rank = result["rank"]
                        post_url = result.get("post_url", "")

                        # ë¸”ë¡œê·¸ ë¶„ì„ (í‚¤ì›Œë“œ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ê°€ì¤‘ì¹˜ ì ìš©)
                        analysis = await analyze_blog(blog_id, keyword)
                        stats = analysis.get("stats", {})
                        index = analysis.get("index", {})
                        breakdown = index.get("score_breakdown", {})
                        c_rank_detail = breakdown.get("c_rank_detail", {})
                        dia_detail = breakdown.get("dia_detail", {})

                        # ê¸€ ë¶„ì„ (ì„ íƒì ) - ê°œì„ ëœ í”¼ì²˜ ì¶”ì¶œ
                        post_features = {}
                        if post_url:
                            try:
                                post_analysis = await analyze_post(post_url, keyword)
                                post_features = {
                                    "title_has_keyword": post_analysis.get("title_has_keyword", False),
                                    "title_keyword_position": post_analysis.get("title_keyword_position", -1),
                                    "content_length": post_analysis.get("content_length", 0),
                                    "image_count": post_analysis.get("image_count", 0),
                                    "video_count": post_analysis.get("video_count", 0),
                                    "keyword_count": post_analysis.get("keyword_count", 0),
                                    "keyword_density": post_analysis.get("keyword_density", 0),
                                    "heading_count": post_analysis.get("heading_count", 0),
                                    "paragraph_count": post_analysis.get("paragraph_count", 0),
                                    "has_map": post_analysis.get("has_map", False),
                                    "has_link": post_analysis.get("has_link", False),
                                    "like_count": post_analysis.get("like_count", 0),
                                    "comment_count": post_analysis.get("comment_count", 0),
                                    "post_age_days": post_analysis.get("post_age_days"),
                                }

                                # post_age_daysê°€ ì—†ìœ¼ë©´ ê²€ìƒ‰ APIì˜ postdateì—ì„œ ê³„ì‚°
                                if post_features["post_age_days"] is None:
                                    post_date_str = result.get("post_date", "") or ""  # YYYYMMDD í˜•ì‹
                                    if not post_date_str:
                                        logger.info(f"[AutoLearn] No post_date in result for {blog_id}: keys={list(result.keys())}")
                                    if post_date_str and len(str(post_date_str)) == 8:
                                        try:
                                            post_date_str = str(post_date_str)
                                            y = int(post_date_str[:4])
                                            m = int(post_date_str[4:6])
                                            d = int(post_date_str[6:8])
                                            post_date = datetime(y, m, d)
                                            post_features["post_age_days"] = (datetime.now() - post_date).days
                                            logger.info(f"[AutoLearn] Got post_age_days from API: {blog_id} = {post_features['post_age_days']} days (date: {post_date_str})")
                                        except Exception as date_err:
                                            logger.debug(f"[AutoLearn] Date parse error: {date_err}")

                                logger.debug(f"Post features: heading={post_features['heading_count']}, paragraph={post_features['paragraph_count']}, age={post_features['post_age_days']}")
                            except Exception as e:
                                logger.debug(f"Post analysis skipped: {e}")

                        # í•™ìŠµ ìƒ˜í”Œ ì €ì¥
                        blog_features = {
                            "c_rank_score": breakdown.get("c_rank", 0),
                            "dia_score": breakdown.get("dia", 0),
                            "context_score": c_rank_detail.get("context", 50),
                            "content_score": c_rank_detail.get("content", 50),
                            "chain_score": c_rank_detail.get("chain", 50),
                            "depth_score": dia_detail.get("depth", 50),
                            "information_score": dia_detail.get("information", 50),
                            "accuracy_score": dia_detail.get("accuracy", 50),
                            "post_count": stats.get("total_posts", 0),
                            "neighbor_count": stats.get("neighbor_count", 0),
                            "visitor_count": stats.get("total_visitors", 0),
                            **post_features
                        }

                        add_learning_sample(
                            keyword=keyword,
                            blog_id=blog_id,
                            actual_rank=actual_rank,
                            predicted_score=index.get("total_score", 0),
                            blog_features=blog_features
                        )

                        blogs_analyzed += 1
                        auto_learning_state["total_blogs_analyzed"] += 1
                        auto_learning_state["samples_since_last_train"] += 1

                        # ë¸”ë¡œê·¸ ê°„ ë”œë ˆì´
                        await asyncio.sleep(config["delay_between_blogs"])

                    except Exception as e:
                        logger.warning(f"[AutoLearn] Blog analysis error: {e}")

                auto_learning_state["total_keywords_learned"] += 1
                logger.info(f"[AutoLearn] Completed {keyword}: {blogs_analyzed} blogs")

                # í‚¤ì›Œë“œ í•™ìŠµ ì™„ë£Œ ê¸°ë¡ (DBì— ì €ì¥)
                mark_keyword_as_learned(keyword, blogs_analyzed)

                # í‚¤ì›Œë“œ ê°„ ë”œë ˆì´
                await asyncio.sleep(config["delay_between_keywords"])

            except Exception as e:
                logger.error(f"[AutoLearn] Keyword error {keyword}: {e}")
                auto_learning_state["errors"].append({
                    "time": datetime.now(timezone.utc).isoformat(),
                    "keyword": keyword,
                    "error": str(e)
                })

        # ìë™ í›ˆë ¨ ì²´í¬
        if auto_learning_state["samples_since_last_train"] >= config["auto_train_threshold"]:
            await run_auto_training()

        auto_learning_state["total_cycles"] += 1
        auto_learning_state["current_keyword"] = None

        # ì—ëŸ¬ ë¡œê·¸ ìµœëŒ€ 20ê°œ ìœ ì§€
        if len(auto_learning_state["errors"]) > 20:
            auto_learning_state["errors"] = auto_learning_state["errors"][-20:]

    except Exception as e:
        logger.error(f"[AutoLearn] Cycle failed: {e}")
        import traceback
        traceback.print_exc()

    finally:
        auto_learning_state["is_running"] = False

        # ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
        interval = get_current_interval()
        auto_learning_state["next_run"] = (
            datetime.now(timezone.utc) + timedelta(minutes=interval)
        ).isoformat()


async def run_auto_training():
    """ìë™ ëª¨ë¸ í›ˆë ¨"""
    global auto_learning_state

    try:
        from database.learning_db import get_learning_samples, get_current_weights, save_current_weights
        from services.learning_engine import instant_adjust_weights

        samples = get_learning_samples(limit=1000)

        if len(samples) < 20:
            logger.info(f"[AutoLearn] Not enough samples for training: {len(samples)}")
            return

        current_weights = get_current_weights()
        if not current_weights:
            return

        logger.info(f"[AutoLearn] Starting auto-training with {len(samples)} samples")

        new_weights, info = instant_adjust_weights(
            samples=samples,
            current_weights=current_weights,
            target_accuracy=95.0,
            max_iterations=30,
            learning_rate=0.03,
            momentum=0.9
        )

        initial_accuracy = info.get("initial_accuracy", 0)
        final_accuracy = info.get("final_accuracy", 0)

        # ì •í™•ë„ê°€ í–¥ìƒë˜ì—ˆì„ ë•Œë§Œ ì €ì¥
        if final_accuracy >= initial_accuracy:
            save_current_weights(new_weights)
            logger.info(f"[AutoLearn] Model improved: {initial_accuracy:.1f}% -> {final_accuracy:.1f}%")
        else:
            logger.info(f"[AutoLearn] Model not improved, rollback: {initial_accuracy:.1f}% -> {final_accuracy:.1f}%")

        auto_learning_state["samples_since_last_train"] = 0

    except Exception as e:
        logger.error(f"[AutoLearn] Training failed: {e}")


async def run_daily_intensive_training():
    """ë§¤ì¼ ëŒ€ê·œëª¨ í›ˆë ¨ (ë” ë§ì€ ìƒ˜í”Œ, ë” ë§ì€ ë°˜ë³µ)"""
    global auto_learning_state

    try:
        from database.learning_db import (
            get_learning_samples, get_current_weights, save_current_weights,
            save_training_session, save_weight_history
        )
        from services.learning_engine import instant_adjust_weights
        import uuid

        config = AUTO_LEARNING_CONFIG
        samples = get_learning_samples(limit=config["daily_training_samples"])

        if len(samples) < 100:
            logger.info(f"[DailyTrain] Not enough samples: {len(samples)}")
            return

        current_weights = get_current_weights()
        if not current_weights:
            return

        session_id = f"daily_{uuid.uuid4().hex[:8]}"
        started_at = datetime.now(timezone.utc).isoformat()

        logger.info(f"[DailyTrain] ğŸš€ Starting intensive training with {len(samples)} samples")

        # ë” ë§ì€ ë°˜ë³µ, ë” ì‘ì€ í•™ìŠµë¥ ë¡œ ì„¸ë°€í•œ ì¡°ì •
        new_weights, info = instant_adjust_weights(
            samples=samples,
            current_weights=current_weights,
            target_accuracy=80.0,  # í˜„ì‹¤ì ì¸ ëª©í‘œ (í‚¤ì›Œë“œë³„ ì •í™•ë„ ê³„ì‚° ì‹œ)
            max_iterations=200,    # ë” ë§ì€ ë°˜ë³µ
            learning_rate=0.02,    # ë” ì‘ì€ í•™ìŠµë¥ 
            momentum=0.95          # ë” ë†’ì€ ëª¨ë©˜í…€
        )

        initial_accuracy = info.get("initial_accuracy", 0)
        final_accuracy = info.get("final_accuracy", 0)
        improvement = final_accuracy - initial_accuracy

        completed_at = datetime.now(timezone.utc).isoformat()

        # ê²°ê³¼ ì €ì¥
        if final_accuracy >= initial_accuracy:
            save_current_weights(new_weights)
            save_weight_history(session_id, new_weights, final_accuracy, len(samples))
            logger.info(f"[DailyTrain] âœ… Model improved: {initial_accuracy:.1f}% -> {final_accuracy:.1f}%")
        else:
            logger.warning(f"[DailyTrain] âš ï¸ Model not improved: {initial_accuracy:.1f}% -> {final_accuracy:.1f}%")

        # ì„¸ì…˜ ì €ì¥
        save_training_session(
            session_id=session_id,
            samples_used=len(samples),
            accuracy_before=initial_accuracy,
            accuracy_after=final_accuracy,
            improvement=improvement,
            duration_seconds=info.get("duration_seconds", 0),
            epochs=info.get("iterations", 0),
            learning_rate=0.02,
            started_at=started_at,
            completed_at=completed_at,
            keywords=list(set(s.get('keyword', '') for s in samples[:100])),
            weight_changes=info.get("weight_changes", {})
        )

        auto_learning_state["last_daily_training"] = completed_at
        auto_learning_state["daily_training_accuracy"] = final_accuracy

        logger.info(f"[DailyTrain] ğŸ“Š Results: {initial_accuracy:.1f}% -> {final_accuracy:.1f}% (Î”{improvement:+.1f}%)")

    except Exception as e:
        logger.error(f"[DailyTrain] Training failed: {e}")
        import traceback
        traceback.print_exc()


class AutoLearningScheduler:
    """ìë™ í•™ìŠµ ìŠ¤ì¼€ì¤„ëŸ¬"""

    def __init__(self):
        self.running = False
        self.thread = None
        self.loop = None

    def start(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
        if self.running:
            logger.info("[AutoLearn] Scheduler already running")
            return

        if not AUTO_LEARNING_CONFIG["enabled"]:
            logger.info("[AutoLearn] Auto learning is disabled in config")
            return

        self.running = True
        auto_learning_state["is_enabled"] = True
        self.thread = threading.Thread(target=self._run_scheduler, daemon=True)
        self.thread.start()
        logger.info("[AutoLearn] Scheduler started")

    def stop(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€"""
        self.running = False
        auto_learning_state["is_enabled"] = False
        if self.thread:
            self.thread.join(timeout=10)
        logger.info("[AutoLearn] Scheduler stopped")

    def enable(self):
        """í•™ìŠµ í™œì„±í™”"""
        auto_learning_state["is_enabled"] = True
        if not self.running:
            self.start()
        logger.info("[AutoLearn] Learning enabled")

    def disable(self):
        """í•™ìŠµ ë¹„í™œì„±í™” (ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ìœ ì§€)"""
        auto_learning_state["is_enabled"] = False
        logger.info("[AutoLearn] Learning disabled")

    def _run_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ë©”ì¸ ë£¨í”„"""
        # ìƒˆ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„±
        self.loop = asyncio.new_event_loop()
        asyncio.set_event_loop(self.loop)

        # ì‹œì‘ ì‹œ ì•½ê°„ì˜ ë”œë ˆì´ (ì„œë²„ ì•ˆì •í™” ëŒ€ê¸°)
        time.sleep(60)

        logger.info("[AutoLearn] Starting first learning cycle...")

        last_daily_training_date = None

        while self.running:
            try:
                current_hour = datetime.now(timezone.utc).hour
                current_date = datetime.now(timezone.utc).date()

                # ë§¤ì¼ ëŒ€ê·œëª¨ í›ˆë ¨ ì²´í¬ (ì§€ì •ëœ ì‹œê°„, í•˜ë£¨ í•œ ë²ˆë§Œ)
                if (current_hour == AUTO_LEARNING_CONFIG["daily_training_hour"] and
                    last_daily_training_date != current_date):
                    logger.info("[AutoLearn] ğŸ¯ Starting daily intensive training...")
                    self.loop.run_until_complete(run_daily_intensive_training())
                    last_daily_training_date = current_date

                if auto_learning_state["is_enabled"]:
                    # ë¹„ë™ê¸° í•™ìŠµ ì‚¬ì´í´ ì‹¤í–‰
                    self.loop.run_until_complete(run_single_learning_cycle())

                # ë‹¤ìŒ ì‚¬ì´í´ê¹Œì§€ ëŒ€ê¸°
                interval = get_current_interval()

                # 1ë¶„ ë‹¨ìœ„ë¡œ ì²´í¬í•˜ë©´ì„œ ëŒ€ê¸° (ë¹ ë¥¸ ì¢…ë£Œ ëŒ€ì‘)
                wait_seconds = interval * 60
                for _ in range(wait_seconds // 60):
                    if not self.running:
                        break
                    time.sleep(60)

                # ë‚¨ì€ ì‹œê°„ ëŒ€ê¸°
                remaining = wait_seconds % 60
                if remaining > 0 and self.running:
                    time.sleep(remaining)

            except Exception as e:
                logger.error(f"[AutoLearn] Scheduler error: {e}")
                time.sleep(300)  # ì—ëŸ¬ ì‹œ 5ë¶„ ëŒ€ê¸°

        self.loop.close()


def get_auto_learning_status() -> Dict:
    """ìë™ í•™ìŠµ ìƒíƒœ ì¡°íšŒ"""
    return {
        "config": AUTO_LEARNING_CONFIG,
        "state": {
            **auto_learning_state,
            "errors_count": len(auto_learning_state["errors"]),
            "recent_errors": auto_learning_state["errors"][-5:] if auto_learning_state["errors"] else []
        }
    }


def update_auto_learning_config(updates: Dict) -> Dict:
    """ìë™ í•™ìŠµ ì„¤ì • ì—…ë°ì´íŠ¸"""
    global AUTO_LEARNING_CONFIG

    allowed_keys = [
        "enabled", "interval_minutes", "keywords_per_cycle",
        "blogs_per_keyword", "delay_between_keywords", "delay_between_blogs",
        "auto_train_threshold", "quiet_hours_start", "quiet_hours_end"
    ]

    for key, value in updates.items():
        if key in allowed_keys:
            AUTO_LEARNING_CONFIG[key] = value

    return AUTO_LEARNING_CONFIG


# ì „ì—­ ìŠ¤ì¼€ì¤„ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
auto_learning_scheduler = AutoLearningScheduler()
